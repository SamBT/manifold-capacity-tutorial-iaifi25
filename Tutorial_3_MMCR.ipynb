{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "25704c8d",
   "metadata": {},
   "source": [
    "# Tutorial 3: Representation Learning with Manifold Capacity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82f2e87f",
   "metadata": {},
   "source": [
    "In our last tutorial, we'll explore how ideas from manifold capacity theory can be adapted for self-supervised representation learning. This tutorial is based on the paper **[Learning Efficient Coding of Natural Images with Maximum Manifold Capacity Representations](https://arxiv.org/abs/2303.03307)** and the accompanying [GitHub codebase](https://github.com/ThomasYerxa/mmcr) from lead author Thomas Yerxa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edf3321d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### GOOGLE COLAB SETUP -- ONLY RUN IF WORKING IN COLAB ###\n",
    "! git clone https://github.com/SamBT/manifold-capacity-tutorial-iaifi25 \n",
    "%cd /content/manifold-capacity-tutorial-iaifi25/\n",
    "! pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f0084e",
   "metadata": {},
   "outputs": [],
   "source": [
    "### GOOGLE COLAB SETUP -- RUN THIS CELL AFTER THE KERNEL RESTARTS ###\n",
    "%cd /content/manifold-capacity-tutorial-iaifi25/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e46c5f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision\n",
    "from torchvision import transforms, models, datasets\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import einops\n",
    "from tqdm import tqdm\n",
    "from helpers import manifold_analysis, manifold_analysis_corr, extractor\n",
    "from helpers.plotting import network_capacity_summary\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d294360",
   "metadata": {},
   "source": [
    "## 3.1 - Augmentation-based self-supervised learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c60a537e",
   "metadata": {},
   "source": [
    "The basic idea behind self-supervised learning (SSL) is to train a neural network $f_\\theta: \\mathbb{R}^N \\to \\mathbb{R}^d$ to learn a low-dimensional ($d \\ll N$) representation of some dataset $X$ **directly from the data**, i.e. without any supervisory signal coming from class labels. The best-known example of SSL is something you probably use every day, and which is helping me write this tutorial: Large Language Models (LLMs). LLMs are trained on \"next token prediction\", essentially predicting the next word in a sentence given the ones that came before. Since no labels are involved this is a self-supervised task, and we can easily scale up the training to eat up every word human beings have ever written.\n",
    "\n",
    "Our setting will be computer vision, where we usually talk about **augmentation-based** SSL and **contrastive learning**. Without going into too much detail, the basic idea is summarized in this image ([source](https://sh-tsang.medium.com/review-simclr-a-simple-framework-for-contrastive-learning-of-visual-representations-5de42ba0bc66)):\n",
    "\n",
    "<p align='center'>\n",
    "    <img src='images/simclr.png' />\n",
    "</p>\n",
    "\n",
    "We train the encoder $f_\\theta$ by sampling a batch of images and creating $k$ augmentations of each one ($k=2$ in the cartoon). The training objective then encourages the network to map augmented views of the same image close to one another in $\\mathbb{R}^d$, while pushing them apart from embeddings of the rest of the images in the batch. For computer vision, the augmentations are usually things like:\n",
    "1. Random crops/rotations/masks\n",
    "2. Resizing\n",
    "3. Gaussian blur/noise\n",
    "4. Color distortion\n",
    "\n",
    "<!--- One of the best-known variants of augmentation-based SSL is **SimCLR**, which uses cosine similarity to compare representations of images. The SimCLR loss function is:\n",
    "\n",
    "$$ \\mathcal{L}_{\\text{SimCLR}}  = -\\sum_{i\\in \\mathcal{B}}\\log\\frac{\\exp(\\text{sim}(\\mathbf{z}_i,\\tilde{\\mathbf{z}}_{i})/\\tau)}{\\sum_{j\\neq i}\\exp(\\text{sim}(\\mathbf{z}_i,\\mathbf{z}_j)/\\tau)} $$\n",
    "\n",
    "where $(\\mathbf{z}_i,\\tilde{\\mathbf{z}}_i)$ are a positive pair (augmentations of the same image) and the sum in the denominator runs over all other pairs in the batch, excluding only self-similarity. The outer sum runs over a batch of data. --->\n",
    "\n",
    "**Note**: Standard practice for SSL is to train an **encoder** $f_\\theta$ and **projection head** $g_\\phi$ simultaneously, where $g:\\mathbb{R}^d \\to \\mathbb{R}^\\ell$ with $\\ell < d$. The self-supervised loss is evaluated on the *projections* $g(f(\\mathbf{x}))$, but the projection head is discarded for downstream tasks. We use this convention throughout this notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e97e82a",
   "metadata": {},
   "source": [
    "## 3.2 - Maximum Manifold Capacity Representations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a7b4749",
   "metadata": {},
   "source": [
    "The Maximim Manifold Capacity Representations ([MMCR](https://arxiv.org/abs/2303.03307)) approach uses ideas from manifold capacity theory to construct a new self-supervised training objective. The basic idea is just what it sounds like: learn an embedding $f_\\theta$ that maximizes the manifold capacity of the learned representations. As we saw in Tutorials 1 and 2, manifold capacity is a great tool for analyzing *trained* networks, but it is very computationally expensive and thus infeasible for direct use as a loss function.\n",
    "\n",
    "To make things tractable we rely on a few facts and simplifying assumptions:\n",
    "1. If the correlation between centroids of a collection of manifolds is low, the capacity can be approximated by $\\phi(R_M\\sqrt{D_M})$, where $\\phi$ is a monotonically decreasing function\n",
    "\n",
    "2. If manifolds have an **elliptical** shape, the radius and dimension can be approximated by $$R_M \\sqrt{\\sum_i \\lambda_i^2}, \\quad D_M = \\frac{\\left(\\sum_i \\lambda_i\\right)^2}{\\sum_i \\lambda_i^2}$$ where $\\lambda_i$ are the eigenvalues of the ellipsoid's covariance matrix. \n",
    "\n",
    "3. Using the above, we can write $$\\alpha = \\phi(\\sum_i \\sigma_i)$$ where $\\sigma_i$ are the **singular values of a matrix containing points on the manifold**. The sum $\\sum_i \\sigma_i$ is known as the **nuclear norm** of this data matrix, which we denote by $||\\cdot||_\\ast$\n",
    "\n",
    "The nuclear norm *is* something we can compute quickly and use in a training objective, which we turn to next"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "574b7973",
   "metadata": {},
   "source": [
    "### 3.2.1 - The MMCR objective\n",
    "The MMCR objective for augmentation-based SSL is summarized in the figure below\n",
    "\n",
    "<p align='center'>\n",
    "    <img src='images/mmcr.png' />\n",
    "</p>\n",
    "\n",
    "We sample a set of augmentations from each image, then compute the centroid of those augmentations in the low-dimensional representation from the neural network. The MMCR objective is designed to maximize the extent of this \"centroid manifold\" by maximizing its nuclear norm. \n",
    "\n",
    "The recipe for training is as follows:\n",
    "1. For a batch of data (i.e. images) $X \\in \\mathbb{R}^{B\\times D}$, sample $k$ augmentations of each image.\n",
    "\n",
    "2. Feed everything through a neural network $f_\\theta$ to acquire a multi-view embedding $Z \\in \\mathbb{R}^{B \\times d \\times k}$. Normalize each embedding to lie on the unit sphere $\\mathbb{S}^{d-1}$\n",
    "\n",
    "3. For each element of the batch, find the **centroid** of its $k$ augmented representations and construct the matrix of centroid $C \\in \\mathbb{R}^{B\\times d}$\n",
    "\n",
    "4. Compute the **MMCR loss**, which seeks to maximize the nuclear norm of this data matrix $$\\mathcal{L}_\\text{MMCR} = -||C||_\\ast$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1f04c6b",
   "metadata": {},
   "source": [
    "In the cell below, complete the implementation of the MMCR loss for a batch of data. You can use `torch.linalg.svdvals` to get the singular values of a matrix.\n",
    "\n",
    "#### **FILL IN CODE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d746cfbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MMCRLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    MMCR Loss function implementation.\n",
    "    Based on the original implementation in the MMCR codebase.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_views=2):\n",
    "        super(MMCRLoss, self).__init__()\n",
    "        self.n_views = n_views # number of augmented views to expect per image\n",
    "    \n",
    "    def forward(self, z):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            z: Tensor of shape (batch_size * n_views, feature_dim)\n",
    "               Contains projections for all views of all images\n",
    "        \n",
    "        Returns:\n",
    "            loss: MMCR loss value\n",
    "        \"\"\"\n",
    "        # Normalize representations to unit sphere\n",
    "        z = ...\n",
    "        \n",
    "        # Reshape to (batch_size, feature_dim, n_views)\n",
    "        z_local = einops.rearrange(z, \"(B N) C -> B C N\", N=self.n_views)\n",
    "        \n",
    "        # Compute centroids: average across views for each image\n",
    "        centroids = ...  # Shape: (batch_size, feature_dim)\n",
    "        \n",
    "        # compute the singular values of the centroid matrix\n",
    "        sigmas = torch.linalg.svdvals(centroids)\n",
    "        # sum them to get the nuclear norm\n",
    "        nuc_norm = ...  # Nuclear norm of the centroids\n",
    "        \n",
    "        # MMCR loss (careful about the sign!)\n",
    "        loss = -nuc_norm \n",
    "        \n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f98e31d2",
   "metadata": {},
   "source": [
    "### 3.3 - Training with MMCR on CIFAR-10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5415e546",
   "metadata": {},
   "source": [
    "Let's see how this works in practice by training on CIFAR-10. We start by defining the random augmentations we'll apply to images during training. We will use the same sequence of transformations as most other self-supervised models (e.g. SimCLR), which consists of:\n",
    "1. A random crop\n",
    "2. Random horizontal flip (50\\% probability)\n",
    "3. Random color jitter (80% probability)\n",
    "4. Random conversion to grayscale (20% probability)\n",
    "\n",
    "For convenience we will define a module that outputs `n_views` random augmentations of an image, which will allow us to experiment training with different numbers of augmentations per image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e41ba0d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar10_mean = (0.49139968, 0.48215827 ,0.44653124)\n",
    "cifar10_std = (0.24703233, 0.24348505, 0.26158768)\n",
    "class MMCRTransform:\n",
    "    \"\"\"\n",
    "    Data augmentation transform that creates multiple views of each image.\n",
    "    This is based on the CIFAR implementation from the MMCR codebase.\n",
    "    \"\"\"\n",
    "    def __init__(self, n_views=2, train_transform=True):\n",
    "        self.n_views = n_views\n",
    "        \n",
    "        if train_transform:\n",
    "            # Training augmentations (stronger)\n",
    "            self.transform = transforms.Compose([\n",
    "                transforms.RandomResizedCrop(32, scale=(0.2, 1.0)),\n",
    "                transforms.RandomHorizontalFlip(p=0.5),\n",
    "                transforms.RandomApply([\n",
    "                    transforms.ColorJitter(brightness=0.4, contrast=0.4, \n",
    "                                         saturation=0.4, hue=0.1)\n",
    "                ], p=0.8),\n",
    "                transforms.RandomGrayscale(p=0.2),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(cifar10_mean, cifar10_std)\n",
    "            ])\n",
    "        else:\n",
    "            # If not training, just do the standard normalization\n",
    "            self.transform = transforms.Compose([\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize(cifar10_mean, cifar10_std)\n",
    "            ])\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        # Create multiple augmented views\n",
    "        views = []\n",
    "        for _ in range(self.n_views):\n",
    "            views.append(self.transform(x))\n",
    "        return torch.stack(views)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d877d93",
   "metadata": {},
   "source": [
    "Let's have a look at what some of these augmentations look like on a sample image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70224cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a sample image from CIFAR-10\n",
    "cifar_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, \n",
    "                                             download=True, transform=None)\n",
    "sample_image, label = cifar_dataset[0]\n",
    "\n",
    "transform = MMCRTransform(n_views=4, train_transform=True)\n",
    "augmented_views = transform(sample_image)\n",
    "\n",
    "# Visualize the augmented views\n",
    "def denormalize(tensor):\n",
    "    mean = torch.tensor(cifar10_mean).view(3, 1, 1)\n",
    "    std = torch.tensor(cifar10_std).view(3, 1, 1)\n",
    "    return tensor * std + mean\n",
    "\n",
    "fig, axes = plt.subplots(1, 5, figsize=(12, 3))\n",
    "\n",
    "# Show original image\n",
    "axes[0].imshow(sample_image)\n",
    "axes[0].set_title('Original')\n",
    "axes[0].axis('off')\n",
    "\n",
    "# Show augmented views\n",
    "for i in range(4):\n",
    "    img = augmented_views[i] * torch.tensor(cifar10_std).view(3,1,1) + torch.tensor(cifar10_mean).view(3,1,1)\n",
    "    img = img.permute(1, 2, 0).clamp(0,1)  # Change to HWC format for visualization.\n",
    "    #img = denormalize(augmented_views[i]).permute(1, 2, 0).clamp(0, 1)\n",
    "    axes[i+1].imshow(img)\n",
    "    axes[i+1].set_title(f'View {i+1}')\n",
    "    axes[i+1].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbe3d220",
   "metadata": {},
   "source": [
    "Define a convenience function for getting dataloaders with the desired number of views. Note the val/test loaders always just use 1 view with no random augmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "344a866a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_loaders(n_views,batch_size,num_workers):\n",
    "    train_transform = MMCRTransform(n_views=n_views, train_transform=True)\n",
    "    test_transform = MMCRTransform(n_views=1, train_transform=False)\n",
    "\n",
    "    # Create datasets with MMCR transforms\n",
    "    train_dataset = torchvision.datasets.CIFAR10( root='./data', train=True, download=True, transform=train_transform)\n",
    "\n",
    "    # For validation we'll use the same dataset but no augmentations\n",
    "    val_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, download=False, transform=test_transform)\n",
    "\n",
    "    test_dataset = torchvision.datasets.CIFAR10(root='./data', train=False, download=False, transform=test_transform)\n",
    "\n",
    "    # Create data loaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers, pin_memory=True)\n",
    "\n",
    "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers, pin_memory=True)\n",
    "\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers, pin_memory=True)\n",
    "\n",
    "    return train_loader, val_loader, test_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd9451dd",
   "metadata": {},
   "source": [
    "Now let's define a `nn.Module` wrapper for our MMCR embedder. We'll use a `resnet18` backbone for the embedder $f_\\theta$ and simple MLP for the projection head $g_\\phi$.\n",
    "\n",
    "#### **FILL IN CODE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa8ae40a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MMCRModel(nn.Module):\n",
    "    \"\"\"\n",
    "    MMCR Model architecture adapted from the original codebase.\n",
    "    Consists of a ResNet encoder + projection head.\n",
    "    \"\"\"\n",
    "    def __init__(self, projector_hidden_dims=[512], projector_output_dim=128):\n",
    "        super(MMCRModel, self).__init__()\n",
    "        \n",
    "        # Create ResNet encoder adapted for CIFAR-10\n",
    "        # We modify the first conv layer for 32x32 images\n",
    "        resnet = torchvision.models.resnet18(pretrained=False)\n",
    "        \n",
    "        # Adapt for CIFAR-10 (32x32 images)\n",
    "        encoder_layers = []\n",
    "        for name, module in resnet.named_children():\n",
    "            if name == \"conv1\":\n",
    "                # Use smaller kernel and stride for CIFAR-10\n",
    "                module = nn.Conv2d(3, 64, kernel_size=3, stride=1, \n",
    "                                 padding=1, bias=False)\n",
    "            if not isinstance(module, nn.Linear) and not isinstance(module, nn.MaxPool2d):\n",
    "                encoder_layers.append(module)\n",
    "        \n",
    "        self.encoder = nn.Sequential(*encoder_layers)\n",
    "        \n",
    "        # Projection head\n",
    "        resnet_output_dim = 512  # Output dimension of resnet18 before the final FC layer\n",
    "        # define a projector network from renset_output_dim to projector_output_dim\n",
    "        # with projector_hidden_dims as intermediate layer widths and whatever internal activaiton you want\n",
    "        # there should be no output activation\n",
    "        self.projector = ...\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x shape: (batch_size * n_views, 3, 32, 32)\n",
    "        batch_size = x.shape[0]\n",
    "        features = self.encoder(x)\n",
    "        features = torch.flatten(features, start_dim=1)  # Flatten spatial dimensions\n",
    "        projections = self.projector(features)\n",
    "        return features, projections"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0f97955",
   "metadata": {},
   "source": [
    "Now we set up some boilerplate for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec43aabd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, train_loader, loss_fn, optimizer, epoch, num_epochs):\n",
    "    \"\"\"Train the model for one epoch\"\"\"\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    pbar = tqdm(train_loader, desc=f'Epoch {epoch+1}/{num_epochs}')\n",
    "    \n",
    "    num_batches = 0\n",
    "    for batch_idx, (images, _) in enumerate(pbar):\n",
    "        # Reshape images: (batch_size, n_views, 3, 32, 32) -> (batch_size * n_views, 3, 32, 32)\n",
    "        images = einops.rearrange(images, \"B N C H W -> (B N) C H W\")\n",
    "        images = images.to(device, non_blocking=True)\n",
    "        \n",
    "        # Forward pass\n",
    "        optimizer.zero_grad()\n",
    "        features, projections = model(images)\n",
    "        loss = loss_fn(projections)\n",
    "        \n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Track metrics\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        # Update progress bar\n",
    "        pbar.set_postfix({\n",
    "            'Loss': f\"{loss.item():.4f}\",\n",
    "        })\n",
    "        num_batches += 1\n",
    "    \n",
    "    # Return average metrics\n",
    "    avg_loss = total_loss / num_batches\n",
    "    return avg_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3bf7e70",
   "metadata": {},
   "source": [
    "Now train the model! Choose some reasonable value for `n_views`, but note that increases the batch by a factor of `n_views` so if you set too high a number the training will be very slow. You should only need to train for a few epochs to get a useable result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00d3a3da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize model, loss function, and optimizer\n",
    "n_views = ...\n",
    "batch_size = ...\n",
    "num_workers = 2\n",
    "projector_hidden_dims = ...\n",
    "projector_output_dim = ...\n",
    "model = MMCRModel(...)\n",
    "loss_fn = MMCRLoss(n_views=n_views)\n",
    "train_loader, val_loader, test_loader = get_loaders(n_views=n_views, batch_size=batch_size, num_workers=num_workers)\n",
    "\n",
    "# Training hyperparameters\n",
    "learning_rate = 1e-3\n",
    "num_epochs = 5  # Reduced for tutorial (original paper uses many epochs)\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# Training tracking\n",
    "train_losses = []\n",
    "\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Train for one epoch\n",
    "    loss = train_one_epoch(model, train_loader, loss_fn, optimizer, epoch, num_epochs)\n",
    "    \n",
    "    # Store metrics\n",
    "    train_losses.append(loss)\n",
    "    \n",
    "    # Print epoch summary\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}: Loss = {loss:.4f}\")\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(train_losses, label=\"Train Loss\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a65027d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the model in case something goes wrong (e.g. kernel crash) - this way you won't need to retrain\n",
    "torch.save(model, 'mmcr_model.pth')\n",
    "model = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba98a874",
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment these lines if you need to reload your trained model\n",
    "#model = torch.load('mmcr_model.pth',weights_only=False)\n",
    "#model = model.to(device)\n",
    "#model=model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aeb4f96",
   "metadata": {},
   "source": [
    "### 3.4 - Analyze MMCR Representations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d57bed5a",
   "metadata": {},
   "source": [
    "Assuming your model training went well, now you can analyze the learned representations the same way as we did in Tutorial 2. Let's start by looking at **class manifolds** - i.e. constructed from multiple exemplars of the same class. We'll look at augmentation manifolds afterwards (these are what the model is really trained on)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dcbf0ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get some test data\n",
    "num_test_images = 200 # shoot for 20 per class\n",
    "\n",
    "num_sampled  = 0\n",
    "images = []\n",
    "labels = []\n",
    "for batch, lab in test_loader:\n",
    "    images.append(batch.squeeze(1)) # remove the view dimension, which should just be 1 for the test set anyway\n",
    "    labels.append(lab)\n",
    "    num_sampled += batch.shape[0]\n",
    "    if num_sampled >= num_test_images:\n",
    "        break\n",
    "images = torch.cat(images, dim=0)[:num_test_images]\n",
    "labels = torch.cat(labels, dim=0)[:num_test_images]\n",
    "\n",
    "activations, layer_names = extractor(model, images, labels, layer_types=['Linear','Conv2d'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf196d5f",
   "metadata": {},
   "source": [
    "Some of the activations will be very high-dimensional since we're using a large backbone. To ease the computational burden a bit we'll project anything with $>5000$ dimensions to 5000 dimensions using a random projection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc056df0",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in tqdm(activations.keys()):\n",
    "    if k == 'labels':\n",
    "        continue\n",
    "    activations[k] = activations[k].reshape(activations[k].shape[0], -1)  # Flatten spatial dimensions\n",
    "    if activations[k].shape[1] > 5000:\n",
    "        # If the feature dimension is too large, reduce it with a random projection\n",
    "        M = np.random.randn(1000,activations[k].shape[1])\n",
    "        M = M / np.linalg.norm(M, axis=1, keepdims=True)\n",
    "        activations[k] = np.matmul(activations[k],M.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d02cef9",
   "metadata": {},
   "source": [
    "Here we copy our function from Tutorial 2 to run the mean-field manifold capacity computations on the activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e0f7c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def capacity_analysis(activations, num_per_label=20):\n",
    "    # Get a sorted list of MNIST labels\n",
    "    labels = sorted(list(set(activations['labels'])))\n",
    "\n",
    "    alphas = {l:[] for l in labels}\n",
    "    radii = {l:[] for l in labels}\n",
    "    dims = {l:[] for l in labels}\n",
    "    mean_alphas = []\n",
    "    mean_radii = []\n",
    "    mean_dims = []\n",
    "\n",
    "    # loop through the raw inputs + each layer of the network\n",
    "    for layer_name in activations.keys():\n",
    "        if layer_name == 'labels':\n",
    "            continue\n",
    "        print(f\"Processing activations for layer: {layer_name}\")\n",
    "        # retrieve corresponding activations\n",
    "        acts = activations[layer_name]\n",
    "\n",
    "        # construct point clouds for each mnist class\n",
    "        point_clouds = []\n",
    "        for l in labels:\n",
    "            mask = activations['labels'] == l\n",
    "            point_clouds.append(acts[mask][:num_per_label].T) # limit the number of samples per class to num_per_label\n",
    "        \n",
    "        # compute manifold capacity, radius, and dimension \n",
    "        alpha, radius, dim = manifold_analysis(point_clouds, kappa=0, n_t=300)\n",
    "\n",
    "        # fill dictionaries with label-specific results\n",
    "        for i,l in enumerate(labels):\n",
    "            alphas[l].append(alpha[i])\n",
    "            radii[l].append(radius[i])\n",
    "            dims[l].append(dim[i])\n",
    "\n",
    "        # compute mean values for alpha, radius, and dimension across the mnist classes\n",
    "        mean_alpha = 1.0/np.mean(1.0/alpha)\n",
    "        mean_radius = np.mean(radius)\n",
    "        mean_dim = np.mean(dim)\n",
    "        \n",
    "        mean_alphas.append(mean_alpha)\n",
    "        mean_radii.append(mean_radius)\n",
    "        mean_dims.append(mean_dim)\n",
    "    return alphas, radii, dims, mean_alphas, mean_radii, mean_dims, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f84ace31",
   "metadata": {},
   "source": [
    "And run the computations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c43bef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas, radii, dims, mean_alphas, mean_radii, mean_dims, labels = capacity_analysis(activations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c037a977",
   "metadata": {},
   "outputs": [],
   "source": [
    "network_capacity_summary(alphas,radii,dims,mean_alphas,mean_radii,mean_dims,layer_names,labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6c13d8e",
   "metadata": {},
   "source": [
    "**Question**: are these results what you expected? Recall that these are class manifolds, not augmentation manifolds -- the model has been given no class-specific information during training! The results here will depend somewhat on how long you trained your model for and/or how many views you gave it per image."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cefb66cc",
   "metadata": {},
   "source": [
    "Let's repeat the same exercise for the **augmentation manifolds**. We'll sample `n_transforms = 20` views of each of the 10 classes (feel free to change this)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb6f90e",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_views = 20\n",
    "aug_manifold_transform = MMCRTransform(n_views=n_views, train_transform=True)\n",
    "aug_dset = torchvision.datasets.CIFAR10(root='./data', train=False, download=False, transform=aug_manifold_transform)\n",
    "aug_loader = DataLoader(aug_dset, batch_size=50, shuffle=True)\n",
    "batch, batch_labels = next(iter(aug_loader))\n",
    "\n",
    "images = []\n",
    "labels = []\n",
    "cifar_labels = np.arange(10)  # CIFAR-10 labels are 0-9\n",
    "for lab in cifar_labels:\n",
    "    x = batch[batch_labels == lab][0] # take first exemplar with its n_views views\n",
    "    images.append(x)\n",
    "    labels.append(lab*np.ones(n_views, dtype=int))\n",
    "images = torch.cat(images, dim=0)\n",
    "labels = torch.tensor(np.concatenate(labels, axis=0))\n",
    "\n",
    "activations, layer_names = extractor(model, images.to(device), labels, layer_types=['Linear','Conv2d'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f32c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in tqdm(activations.keys()):\n",
    "    if k == 'labels':\n",
    "        continue\n",
    "    activations[k] = activations[k].reshape(activations[k].shape[0], -1)  # Flatten spatial dimensions\n",
    "    if activations[k].shape[1] > 5000:\n",
    "        # If the feature dimension is too large, reduce it with a random projection\n",
    "        M = np.random.randn(1000,activations[k].shape[1])\n",
    "        M = M / np.linalg.norm(M, axis=1, keepdims=True)\n",
    "        activations[k] = np.matmul(activations[k],M.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a0cc84a",
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas, radii, dims, mean_alphas, mean_radii, mean_dims, labels = capacity_analysis(activations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "970af579",
   "metadata": {},
   "outputs": [],
   "source": [
    "network_capacity_summary(alphas,radii,dims,mean_alphas,mean_radii,mean_dims,layer_names,labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b8b5cf4",
   "metadata": {},
   "source": [
    "Are these results more in line with what you'd expect? Does this make sense in light of the training setup?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec38b518",
   "metadata": {},
   "source": [
    "#### **Exercises**\n",
    "Here are a few suggestions for exploring MMCR a bit further:\n",
    "1. Train a linear (or very shallow) classifier on the learned embeddings -- how well does it do? This is a standard evaluation technique for SSL.\n",
    "\n",
    "2. Use your favorite dimensionality reduction technique (PCA, UMAP, t-SNE, etc.) to project MMCR embeddings to 2D and visualize them. Try looking at both class manifolds (embeddings of many different elements of each class) and augmentation manifolds (one exemplar from each class with $N$ random augmentations applied).\n",
    "\n",
    "3. Try different training configurations -- in particular, try varying the number of views (augmentations) supplied for each image. This is one of the most important parameters for MMCR, as it determines the number of points contributing to the computation of centroid manifolds in the loss.\n",
    "\n",
    "4. Train another model using SimCLR (loss function implemented below) and compare its performance to MMCR. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f9cd7cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimCLRLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    SimCLR loss function implementation.\n",
    "    Adapted from SupCon loss https://github.com/HobbitLong/SupContrast/blob/master/losses.py\n",
    "    \"\"\"\n",
    "    def __init__(self, n_views, temperature=0.5):\n",
    "        super(MMCRLoss, self).__init__()\n",
    "        self.n_views = n_views # number of augmented views to expect per image\n",
    "        self.temperature = temperature\n",
    "    \n",
    "    def forward(self, z):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            z: Tensor of shape (batch_size * n_views, feature_dim)\n",
    "               Contains projections for all views of all images\n",
    "        \n",
    "        Returns:\n",
    "            loss: MMCR loss value\n",
    "        \"\"\"\n",
    "        # Normalize representations to unit sphere\n",
    "        z = F.normalize(z, dim=-1)\n",
    "        \n",
    "        # Reshape to (batch_size, n_views, feature_dim)\n",
    "        z = einops.rearrange(z, \"(B N) C -> B N C\", N=self.n_views)\n",
    "        batch_size = z.shape[0]\n",
    "        \n",
    "        mask = torch.eye(batch_size,dtype=torch.float32).to(z.device)\n",
    "        z_flat = torch.cat(torch.unbind(z,dim=1),dim=0)\n",
    "        cos_sim_over_t = torch.div(torch.matmul(z_flat, z_flat.T), self.temperature)\n",
    "        \n",
    "        # for numerical stability, we subtract the max value in each row\n",
    "        sim_max, _ = torch.max(cos_sim_over_t, dim=1, keepdim=True)\n",
    "        logits = cos_sim_over_t - sim_max.detach()\n",
    "\n",
    "        # tile mask\n",
    "        mask = mask.repeat(self.n_views, self.n_views)\n",
    "        # mask out self-contrast\n",
    "        logits_mask = torch.scatter(\n",
    "            torch.ones_like(mask),\n",
    "            1,\n",
    "            torch.arange(batch_size * self.n_views).view(-1, 1).to(mask.device),\n",
    "            0\n",
    "        ).to(mask.device)\n",
    "        mask = mask * logits_mask\n",
    "        \n",
    "        # compute log prob\n",
    "        exp_logits = torch.exp(logits) * logits_mask # no self contrast\n",
    "        log_prob = logits - torch.log(exp_logits.sum(1, keepdim=True))\n",
    "        mask_pos_pairs = mask.sum(1)\n",
    "        mean_log_prob_pos = (mask * log_prob).sum(1) / mask_pos_pairs\n",
    "\n",
    "        # loss\n",
    "        loss = -mean_log_prob_pos\n",
    "        loss = loss.view(self.n_views, batch_size).mean()\n",
    "\n",
    "        return loss\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iaifi25",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
