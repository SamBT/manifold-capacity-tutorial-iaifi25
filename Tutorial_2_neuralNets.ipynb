{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5aa05146-099d-44c4-91fe-1d74267ec492",
   "metadata": {},
   "source": [
    "# Tutorial 2: Analyzing Neural Networks\n",
    "\n",
    "In this part of the tutorial we'll use the manifold capacity framework to analyze the representations learned by neural networks as we train them on classification tasks. The studies we'll do in this tutorial are inspired by those presented in **[Separability and geometry of object manifolds in deep neural networks](https://www.nature.com/articles/s41467-020-14578-5)**, the follow-up to the original manifold capacity paper that applied the theoretical ideas to real-world machine learning problems. \n",
    "\n",
    "We'll use the official code implementation for capacity calculations, so you won't need to copy over your work from Tutorial 1 (but feel free to use it if you want!). We'll be analyzing image classifiers and investigating how the geometry of image representations changes as they are passed through the layers of a trained network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e5eefbb",
   "metadata": {},
   "source": [
    "**Run these cells first!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47458eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "### GOOGLE COLAB SETUP -- ONLY RUN IF WORKING IN COLAB ###\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')xw\n",
    "! git clone https://github.com/SamBT/manifold-capacity-tutorial-iaifi25 \n",
    "%cd /content/manifold-capacity-tutorial-iaifi25/\n",
    "! pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6938cd6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import datasets, transforms\n",
    "from tqdm import tqdm\n",
    "from helpers import manifold_analysis_corr, manifold_analysis, extractor\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af6d10c2",
   "metadata": {},
   "source": [
    "## 2.1 - MNIST analysis\n",
    "Let's start simple by training a classifier on $28 \\times 28$ images of handwritten digits from MNIST. We'll train both a standard MLP and a convolutional neural network and see how they differ using our manifold capacity tools. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "079177e8",
   "metadata": {},
   "source": [
    "Let's start with the boring stuff -- dataloaders, train/test/val splits, etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b24d72a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the transformations\n",
    "mnist_mean = 0.1307\n",
    "mnist_std = 0.3081\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((mnist_mean,), (mnist_std,))  # MNIST mean and std\n",
    "])\n",
    "\n",
    "# Load datasets\n",
    "train_dataset = datasets.MNIST('./data', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.MNIST('./data', train=False, transform=transform)\n",
    "\n",
    "# Split train into train and validation\n",
    "train_size = int(0.8 * len(train_dataset))\n",
    "val_size = len(train_dataset) - train_size\n",
    "train_dataset, val_dataset = torch.utils.data.random_split(train_dataset, [train_size, val_size])\n",
    "\n",
    "# useful for later\n",
    "mnist_labels = np.arange(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8156391",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to visualize samples from each class\n",
    "def visualize_mnist_samples(dataset, num_samples_per_class=5):\n",
    "    # Create a figure with subplots for each class (0-9)\n",
    "    fig, axes = plt.subplots(10, num_samples_per_class, figsize=(8, 8))\n",
    "    \n",
    "    # Dictionary to keep track of samples per class\n",
    "    class_samples = {i: 0 for i in range(10)}\n",
    "    \n",
    "    # Iterate through the dataset until we have enough samples for each class\n",
    "    for idx in range(len(dataset)):\n",
    "        # Get image and label\n",
    "        img, label = dataset[idx]\n",
    "        \n",
    "        # If we haven't collected enough samples for this class\n",
    "        if class_samples[label] < num_samples_per_class:\n",
    "            # Convert tensor to numpy and reshape\n",
    "            img_np = img.squeeze().numpy()\n",
    "            \n",
    "            # Plot the image\n",
    "            axes[label, class_samples[label]].imshow(img_np, cmap='gray')\n",
    "            axes[label, class_samples[label]].axis('off')\n",
    "            \n",
    "            # Increment count for this class\n",
    "            class_samples[label] += 1\n",
    "        \n",
    "        # Check if we have collected enough samples for all classes\n",
    "        if all(count >= num_samples_per_class for count in class_samples.values()):\n",
    "            break\n",
    "    \n",
    "    # Set figure title\n",
    "    plt.suptitle('MNIST samples', fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.subplots_adjust(top=0.95)\n",
    "    plt.show()\n",
    "\n",
    "# Visualize samples from each class\n",
    "visualize_mnist_samples(train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "544cecf6",
   "metadata": {},
   "source": [
    "Now let's define a basic MLP for classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c33ef1a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dims, output_dim, activation=nn.SiLU(), dropout=0.0):\n",
    "        super(MLP, self).__init__()\n",
    "        layers = []\n",
    "        prev_dim = input_dim\n",
    "        for hidden_dim in hidden_dims:\n",
    "            layers.append(nn.Linear(prev_dim, hidden_dim))\n",
    "            layers.append(activation)\n",
    "            if dropout > 0:\n",
    "                layers.append(nn.Dropout(dropout))\n",
    "            prev_dim = hidden_dim\n",
    "        layers.append(nn.Linear(prev_dim, output_dim))\n",
    "        self.model = nn.Sequential(*layers)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        if len(x.shape) > 2:\n",
    "            x = x.view(x.size(0), -1) # flatten if given multidimensional input (image)\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08cd486c",
   "metadata": {},
   "source": [
    "Next we define a generic training loop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4115a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, epochs=30, patience=5, learning_rate=1e-3, weight_decay=1e-5):\n",
    "    # Set device\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = model.to(device)\n",
    "    \n",
    "    # Loss function and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "    \n",
    "    # For tracking metrics\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    best_val_loss = float('inf')\n",
    "    epochs_no_improve = 0\n",
    "    best_model_weights = None\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in tqdm(range(epochs), desc=\"Training Progress\"):\n",
    "        # Training phase\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for inputs, labels in train_loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            # Backward pass and optimize\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Statistics\n",
    "            epoch_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "        \n",
    "        epoch_train_loss = epoch_loss / len(train_loader)\n",
    "        epoch_train_acc = correct / total\n",
    "        train_losses.append(epoch_train_loss)\n",
    "        \n",
    "        # Validation phase\n",
    "        model.eval()\n",
    "        epoch_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in val_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                outputs = model(inputs)\n",
    "                loss = criterion(outputs, labels)\n",
    "                # Statistics\n",
    "                epoch_loss += loss.item()\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "        epoch_val_loss = epoch_loss / len(val_loader)\n",
    "        epoch_val_acc = correct / total\n",
    "        val_losses.append(epoch_val_loss)\n",
    "        \n",
    "        # Update tqdm postfix with metrics\n",
    "        tqdm.write(f\"Epoch {epoch+1}/{epochs} - Train loss: {epoch_train_loss:.4f}, Train acc: {epoch_train_acc:.4f}, \"\n",
    "                  f\"Val loss: {epoch_val_loss:.4f}, Val acc: {epoch_val_acc:.4f}\")\n",
    "        \n",
    "        # Check if this is the best model\n",
    "        if epoch_val_loss < best_val_loss:\n",
    "            best_val_loss = epoch_val_loss\n",
    "            best_model_weights = model.state_dict().copy()\n",
    "            epochs_no_improve = 0\n",
    "        else:\n",
    "            epochs_no_improve += 1\n",
    "            \n",
    "        # Early stopping\n",
    "        if patience > 0 and epochs_no_improve >= patience:\n",
    "            print(\"Early stop\")\n",
    "            break\n",
    "    \n",
    "    # Load the best model weights\n",
    "    if best_model_weights:\n",
    "        model.load_state_dict(best_model_weights)\n",
    "    \n",
    "    # Plot training and validation loss\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(range(1, len(train_losses)+1), train_losses, label='Train Loss')\n",
    "    plt.plot(range(1, len(val_losses)+1), val_losses, label='Val Loss')\n",
    "    plt.xlabel('Epochs')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    \n",
    "    return model, train_losses, val_losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69a7ccd5",
   "metadata": {},
   "source": [
    "### 2.1.1 - Analyze an MLP trained on MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be8da592",
   "metadata": {},
   "source": [
    "First things first, define an `MLP` (with whatever hyperparameters you want) and, **before training it** use the `extractor` helper function (imported above) to evaluate internal activations on a sample of test set images (1000 or so should be fine, no need to do the whole dataset). The extractor function returns the following dictionary:\n",
    "\n",
    "```\n",
    "activations = {\n",
    "   \"inputs\": your input data (flattened), shape (B,d_input),\n",
    "   \"labels\": sample labels,\n",
    "   for i = 1,..., n_layers:\n",
    "       \"linear_i\", \"conv_i\", etc.: activations, # name depends on layer type\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f951efc6",
   "metadata": {},
   "source": [
    "#### **FILL IN CODE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d583d6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize an MLP with whatever hyperparameters you think are appropriate\n",
    "# the output dimension should be 10, since we're doing MNIST classification\n",
    "input_dim = 28 * 28  # MNIST images are 28x28 pixels\n",
    "hidden_dims = ...\n",
    "output_dim = 10  # 10 classes (digits 0-9)\n",
    "mnist_model = MLP(...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe24a134",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a dataloader for the test set and extract activations for 1000 test samples\n",
    "num_test_samples = 1000\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=num_test_samples)\n",
    "test_images, test_labels = next(iter(test_loader))\n",
    "layer_types = ['Linear'] # type of layers to extract activations from; our MLP only has Linear layers\n",
    "\n",
    "activations_untrained, layer_names = extractor(mnist_model, test_images, test_labels, layer_types=layer_types)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72cff6e0",
   "metadata": {},
   "source": [
    "Before training, let's use our manifold capacity tools to analyze the input data and the untrained MLP. Samples from each MNIST digit (10 classes, digits 0-9) correspond to our manifold point clouds. We want to compute the manifold capacity, radius, and dimension for each of these classes at each level of processing (input data, 1st layer activations, 2nd layer activations, etc.) and see how they evolve.\n",
    "\n",
    "**Question**: The network is untrained -- do you expect to see any trends in capacity/radius/dimension as we pass through layers of the network?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9fe0ccb",
   "metadata": {},
   "source": [
    "### **FILL IN CODE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30fd42cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def capacity_analysis(activations, num_per_label=20):\n",
    "    # Get a sorted list of MNIST labels\n",
    "    labels = sorted(list(set(activations['labels'])))\n",
    "\n",
    "    alphas = {l:[] for l in labels}\n",
    "    radii = {l:[] for l in labels}\n",
    "    dims = {l:[] for l in labels}\n",
    "    mean_alphas = []\n",
    "    mean_radii = []\n",
    "    mean_dims = []\n",
    "\n",
    "    # loop through the raw inputs + each layer of the network\n",
    "    for layer_name in activations.keys():\n",
    "        if layer_name == 'labels':\n",
    "            continue\n",
    "        print(f\"Processing activations for layer: {layer_name}\")\n",
    "        # retrieve corresponding activations\n",
    "        acts = activations[layer_name]\n",
    "\n",
    "        # construct point clouds for each mnist class\n",
    "        point_clouds = []\n",
    "        for l in labels:\n",
    "            mask = activations['labels'] == l\n",
    "            # grab samples with label l using the mask; only grab num_per_label of them though\n",
    "            selected = ... # grab samples with mask; limit to num_per_label\n",
    "            # add them to the point cloud list (careful about the shape!)\n",
    "            # recall that the manifold analysis wants shape (dimensionality, number of points!)\n",
    "            point_clouds.append(...)\n",
    "        \n",
    "        # compute manifold capacity, radius, and dimension \n",
    "        alpha, radius, dim = manifold_analysis(point_clouds, kappa=0, n_t=300)\n",
    "\n",
    "        # fill dictionaries with label-specific results\n",
    "        for i,l in enumerate(labels):\n",
    "            alphas[l].append(alpha[i])\n",
    "            radii[l].append(radius[i])\n",
    "            dims[l].append(dim[i])\n",
    "\n",
    "        # compute mean values for alpha, radius, and dimension across the mnist classes\n",
    "        mean_alpha = 1.0/np.mean(1.0/alpha)\n",
    "        mean_radius = np.mean(radius)\n",
    "        mean_dim = np.mean(dim)\n",
    "        \n",
    "        mean_alphas.append(mean_alpha)\n",
    "        mean_radii.append(mean_radius)\n",
    "        mean_dims.append(mean_dim)\n",
    "    return alphas, radii, dims, mean_alphas, mean_radii, mean_dims, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e631f3ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas, radii, dims, mean_alphas, mean_radii, mean_dims, labels = capacity_analysis(activations_untrained, num_per_label=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59692696",
   "metadata": {},
   "source": [
    "Now let's plot capacity, radius, and dimension as a function of \"depth\" -- i.e. the x-axis will be the layer of the MLP, with the first entry corresponding to raw inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b358a53f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers.plotting import network_capacity_summary\n",
    "\n",
    "network_capacity_summary(alphas,radii,dims,mean_alphas,mean_radii,mean_dims,layer_names,labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbccc463",
   "metadata": {},
   "source": [
    "There shouldn't be any intelligible trend here, but this is to be expected -- we haven't trained the network yet! Note that the average capacity is pretty low, indicating that the raw pixel values (and a random processing of them by an untrained MLP) is not a super efficient way to encode them! \n",
    "\n",
    "**Question**: do you notice anything about the capacity/radius/dimension of the individual digits? What about the number 1?\n",
    "\n",
    "**Question**: why does the dimension dip in the final layer?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6790bacc",
   "metadata": {},
   "source": [
    "Now, let's train the model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cb07fb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define train/val/test loaders \n",
    "batch_size = 1024\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "# Train the model\n",
    "trained_model, train_losses, val_losses = train_model(\n",
    "    model=mnist_model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    epochs=20,\n",
    "    patience=5,\n",
    "    learning_rate=0.001\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "357c8156",
   "metadata": {},
   "source": [
    "Now let's repeat the same analysis we did above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ba288dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a dataloader for the test set and extract activations for 1000 test samples\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=1000)\n",
    "test_images, test_labels = next(iter(test_loader))\n",
    "layer_types = ['Linear']\n",
    "\n",
    "activations_trained, layer_names = extractor(trained_model, test_images, test_labels, layer_types=layer_types)\n",
    "alphas, radii, dims, mean_alphas, mean_radii, mean_dims, labels = capacity_analysis(activations_trained, num_per_label=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b758518d",
   "metadata": {},
   "outputs": [],
   "source": [
    "network_capacity_summary(alphas,radii,dims,mean_alphas,mean_radii,mean_dims,layer_names,mnist_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dece89d7",
   "metadata": {},
   "source": [
    "What differences do you notice relative to the untrained model? Does this make sense? What can we conclude about the way the neural network has learned to process input images from each digit class?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d1bb374",
   "metadata": {},
   "source": [
    "### 2.1.2 - Augmentation manifolds\n",
    "As we discussed in lecture and Tutorial 1, one of the motivating problems for manifold capacity theory is invariant object recognition - our ability to quickly identify classes of objects in diverse contexts. This suggests that we learn efficient representations of **augmentation manifolds**, i.e. the collection of neural activations corresponding to many different views of an objects. We can directly probe these manifolds using our trained MNIST classifier. We'll do the following:\n",
    "\n",
    "1. Choose some parametrized family of image augmentations\n",
    "\n",
    "2. Take one exemplar from each digit, and for each generate an augmentation manifold by sampling some range of transformations.\n",
    "\n",
    "3. Run our manifold capacity analysis on the augmentation manifolds across the layers of our neural network, and see if they become any more/less compact.\n",
    "\n",
    "We'll try this for two kinds of augmentation: shears and rotations.\n",
    "\n",
    "Let's begin with shears"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53ffd018",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms.functional import affine\n",
    "\n",
    "img,label = train_dataset[271] # choose a random image\n",
    "\n",
    "shears = np.arange(-30,35,5) # sample some shear angles\n",
    "\n",
    "fig,axes = plt.subplots(1, len(shears), figsize=(20, 5))\n",
    "for i, shear in enumerate(shears):\n",
    "    # Apply shear transformation\n",
    "    img_sheared = affine(img, angle=0, translate=(0, 0), scale=1.0, shear=shear, fill=-mnist_mean/mnist_std)\n",
    "    axes[i].imshow(img_sheared.squeeze().numpy(), cmap='gray')\n",
    "    axes[i].set_title(f'${shear}^\\\\circ$',fontsize=12)\n",
    "    axes[i].axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1972dab1",
   "metadata": {},
   "source": [
    "Now let's curate a dataset. We want to take one exemplar from each digit class and create an associated augmentation manifold of shear transformations (note that this is different from before, where each class point cloud was just a collection of randomly sampled images from that class)\n",
    "\n",
    "#### **FILL IN CODE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a2825fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=100)\n",
    "test_images, test_labels = next(iter(test_loader))\n",
    "one_per_class = [test_images[test_labels == l][0] for l in mnist_labels]\n",
    "\n",
    "# apply shears to each exemplar, create new label set\n",
    "shears = ... # define some reasonably dense sampling of shear angles; you don't need too many, maybe 20-30 \n",
    "shear_imgs = []\n",
    "shear_labels = []\n",
    "for shear in shears:\n",
    "    for img, label in zip(one_per_class, mnist_labels):\n",
    "        # Apply shear transformation\n",
    "        img_sheared = ...\n",
    "        shear_imgs.append(img_sheared.unsqueeze(0))  # add batch dimension\n",
    "        shear_labels.append(label)\n",
    "shear_imgs = torch.cat(shear_imgs, dim=0)\n",
    "shear_labels = torch.tensor(shear_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ffaa64",
   "metadata": {},
   "source": [
    "Let's analyze the augmentation manifolds using the tools we wrote above and our trained mnist classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48723e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "activations_shear, layer_names = extractor(trained_model, shear_imgs, shear_labels, layer_types=['Linear'])\n",
    "alphas, radii, dims, mean_alphas, mean_radii, mean_dims, labels = capacity_analysis(activations_shear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ccdd550",
   "metadata": {},
   "outputs": [],
   "source": [
    "network_capacity_summary(alphas,radii,dims,mean_alphas,mean_radii,mean_dims,layer_names,mnist_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f45cd26",
   "metadata": {},
   "source": [
    "What do you notice about these results? Is it what you expected? Shears are fairly \"natural\" transformations for handwritten digits -- everyone writes numbers a little differently, and a sharply angled \"1\" or \"7\" is not out of the realm of possibility (and probably exists in the MNIST training set!). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84f13e81",
   "metadata": {},
   "source": [
    "**Exercise**: \n",
    "\n",
    "Repeat this analysis with **rotations** between e.g. -90 and 90 degrees. These are a more \"aggressive\" augmentation relative to shears, and relative to the range of variation for each digit that the model would've seen in training. You can do this using the same `affine` transformation function we used above, just change the arguments. What's different this time?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffe972db",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.transforms.functional import affine\n",
    "img,label = train_dataset[271]\n",
    "\n",
    "angles = list(np.linspace(-90,90,20))\n",
    "fig,axes = plt.subplots(1, len(angles), figsize=(20, 5))\n",
    "for i, angle in enumerate(angles):\n",
    "    # Apply shear transformation\n",
    "    img_sheared = affine(img, angle=angle, translate=(0, 0), scale=1.0, shear=0, fill=-mnist_mean/mnist_std)\n",
    "    axes[i].imshow(img_sheared.squeeze().numpy(), cmap='gray')\n",
    "    axes[i].axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39ec2658",
   "metadata": {},
   "source": [
    "#### **FILL IN CODE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf15632",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=100)\n",
    "test_images, test_labels = next(iter(test_loader))\n",
    "one_per_class = [test_images[test_labels == l][0] for l in mnist_labels]\n",
    "\n",
    "# apply shears to each exemplar, create new label set\n",
    "angles = ...\n",
    "rot_imgs = []\n",
    "rot_labels = []\n",
    "for angle in angles:\n",
    "    for img, label in zip(one_per_class, mnist_labels):\n",
    "        # Apply rotation transformation\n",
    "        img_rotated = ...\n",
    "        rot_imgs.append(img_rotated.unsqueeze(0))  # add batch dimension\n",
    "        rot_labels.append(label)\n",
    "rot_imgs = torch.cat(rot_imgs, dim=0)\n",
    "rot_labels = torch.tensor(rot_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f722e2c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "activations_rot, layer_names = extractor(trained_model, rot_imgs, rot_labels, layer_types=['Linear'])\n",
    "alphas, radii, dims, mean_alphas, mean_radii, mean_dims, labels = capacity_analysis(activations_rot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d875a275",
   "metadata": {},
   "outputs": [],
   "source": [
    "network_capacity_summary(alphas,radii,dims,mean_alphas,mean_radii,mean_dims,layer_names,labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4956ba94",
   "metadata": {},
   "source": [
    "### 2.1.3 - CNN analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31202b9b",
   "metadata": {},
   "source": [
    "As you likely know, MLPs are generally pretty bad for image classification (MNIST just happens to be an \"easy\" dataset). Convolutional neural networks (CNNs) are a much better choice, and most of the best image classifiers from the past decade (e.g. ResNet) are CNNs. Convolutions also have nice geometric properties: they are **translation equivariant**, and when combined with some kind of global pooling (e.g. mean/max) a CNN can be translation **invariant**. \n",
    "\n",
    "In the cell below we define a basic CNN architecture sufficient for MNIST classification. Try training it and doing the same analysis as we did for the MLP above. In particular, try constructing an augmentation manifold from **translations**, and track how the capacity/radius/dimension of this manifold vary through layers of the network (particularly before/after the global pooling)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1190bcb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        \"\"\"\n",
    "        Convolutional Neural Network for MNIST classification\n",
    "        \"\"\"\n",
    "        super(CNN, self).__init__()\n",
    "        self.model = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=3, padding=1),  # 28x28 -> 28x28\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1), # 28x28 -> 28x28\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),                             # 28x28 -> 14x14\n",
    "\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1), # 14x14 -> 14x14\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(128, 128, kernel_size=3, padding=1),# 14x14 -> 14x14\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2),                              # 14x14 -> 7x7\n",
    "\n",
    "            # pooling\n",
    "            nn.AdaptiveAvgPool2d((1, 1)),                 # 7x7 -> 1x1\n",
    "\n",
    "            # fully connected layer,\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(128, 10)\n",
    "        )\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass through the network\"\"\"\n",
    "        return self.model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a15f30d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_model = CNN()\n",
    "# define train/val/test loaders, choose a batch size\n",
    "batch_size = 1024\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size)\n",
    "# Train the model\n",
    "trained_cnn, train_losses, val_losses = train_model(\n",
    "    model=cnn_model,\n",
    "    train_loader=train_loader,\n",
    "    val_loader=val_loader,\n",
    "    epochs=20,``\n",
    "    patience=5,\n",
    "    learning_rate=0.001\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80719c9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a dataloader for the test set and extract activations for 1000 test samples\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=1000)\n",
    "test_images, test_labels = next(iter(test_loader))\n",
    "cnn_acts_trained, layer_names = extractor(trained_cnn, test_images, test_labels, layer_types=['Linear','Conv2d'])\n",
    "\n",
    "alphas, radii, dims, mean_alphas, mean_radii, mean_dims, labels = capacity_analysis(cnn_acts_trained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d21b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "network_capacity_summary(alphas,radii,dims,mean_alphas,mean_radii,mean_dims,layer_names,labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84114a57",
   "metadata": {},
   "source": [
    "#### **Exercise**\n",
    "\n",
    "Repeat the augmentation manifold analysis from above, this time including **translations** in addition to rotations/shears (use periodic boundaries or keep the translations moderate). What do you notice? How does this relate to the CNN architecture?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20e6ada8",
   "metadata": {},
   "source": [
    "## 2.2 - Analyze a pre-trained ResNet on CIFAR-10\n",
    "MNIST is just about the simplest image dataset out there. We can repeat this exercise on [CIFAR-10](https://www.cs.toronto.edu/~kriz/cifar.html), a slightly more complicated dataset made from 32x32 color images of real-world objects. We won't bother training a classifier this time, instead using a pre-trained ResNet from [this repository](https://github.com/huyvnphan/PyTorch_CIFAR10)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "319146a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = torch.hub.load(\"chenyaofo/pytorch-cifar-models\", \"cifar10_resnet20\", pretrained=True)\n",
    "model.eval()\n",
    "\n",
    "cifar10_mean = [0.4914, 0.4822, 0.4465]\n",
    "cifar10_std = [0.2023, 0.1994, 0.2010]\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=cifar10_mean, std=cifar10_std),\n",
    "])\n",
    "dataset = datasets.CIFAR10('./data',train=False, transform=transform, download=True)\n",
    "ciar10_labels = np.arange(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8bc5133",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a dataloader for the test set and extract activations for 1000 test samples\n",
    "test_loader = torch.utils.data.DataLoader(dataset, batch_size=200, shuffle=True)\n",
    "test_images, test_labels = next(iter(test_loader))\n",
    "imagenette_labels = sorted(list(set(test_labels.numpy())))\n",
    "activations_resnet, layer_names = extractor(model, test_images, test_labels, layer_types=['Linear','Conv2d'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "614752c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "alphas, radii, dims, mean_alphas, mean_radii, mean_dims, labels = capacity_analysis(activations_resnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79797c63",
   "metadata": {},
   "outputs": [],
   "source": [
    "network_capacity_summary(alphas,radii,dims,mean_alphas,mean_radii,mean_dims,layer_names,labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6cb30a6",
   "metadata": {},
   "source": [
    "What trends do you notice here? How do they compare to what you observed for the two \"simple\" models above?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55293659",
   "metadata": {},
   "source": [
    "#### **Exercise**\n",
    "\n",
    "Investigate a few \"augmentation manifolds\" as above"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iaifi25",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
