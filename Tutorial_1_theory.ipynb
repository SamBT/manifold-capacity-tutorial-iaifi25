{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0912a9a8",
   "metadata": {},
   "source": [
    "# Tutorial 1: Manifold Capacity Theory & Implementation\n",
    "Run the cells below before you do anything else. Don't run the colab cells if you're not working in colab!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "617039a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "### GOOGLE COLAB SETUP -- ONLY RUN IF WORKING IN COLAB ###\n",
    "! git clone https://github.com/SamBT/manifold-capacity-tutorial-iaifi25 \n",
    "%cd /content/manifold-capacity-tutorial-iaifi25/\n",
    "! pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e27826d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### GOOGLE COLAB SETUP -- RUN THIS CELL AFTER THE KERNEL RESTARTS ###\n",
    "%cd /content/manifold-capacity-tutorial-iaifi25/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b30cc950",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "#### RUN THIS CELL FIRST ####\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import cvxopt\n",
    "cvxopt.solvers.options['show_progress'] = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b524802",
   "metadata": {},
   "source": [
    "## 1.1 - Lecture recap\n",
    "\n",
    "In lecture, we discussed the concept of a **neural manifold**: the ensemble of neural activations linked to the perception of a specific kind of object, e.g. a cat or a dog. The notion of \"invariant object recognition\" - our ability to quickly recognize a cat or a dog in many different contexts (\"views\") -- suggests that ours brain processes visual signals into neural manifolds that are well-separated in neural activation space, requiring little additional processing for us to \"classify\" them into a known category. This idea is illustrated in the figure below ([source](https://www.nature.com/articles/s41467-020-14578-5))\n",
    "\n",
    "<p align='center'>\n",
    "    <img src=\"images/neural_manifold.png\" style=\"width:50%\">\n",
    "</p>\n",
    "\n",
    "In mathematical language, we characterize such an arrangement as **linearly separable** - i.e. I can draw a hyperplane in neural activation space such that each manifold lies fully on one side or the other. In more colloquial language, we might describe the manifolds as \"untangled\".\n",
    "\n",
    "Given this mathematical picture, a number of questions naturally come to mind:\n",
    "\n",
    "1. How many manifolds can be \"packed\" into neural activation spaces such that they remain linearly separable?\n",
    "2. How does this packing relate to the intrinsic dimension or extent of the neural manifolds?\n",
    "3. How can we mathematically analyze the linear separability of *manifolds* as opposed to *points*, which is the usual setting for linear classification problems (e.g. [perceptrons](https://en.wikipedia.org/wiki/Perceptron) or [support vector machines](https://en.wikipedia.org/wiki/Support_vector_machine))?\n",
    "\n",
    "These are the some of questions that [manifold capacity theory](https://journals.aps.org/prx/abstract/10.1103/PhysRevX.8.031003) was developed to answer. \n",
    "\n",
    "Manifold capacity turns out to be useful beyond the scope of neuroscience, most notably in AI. Neural networks trained to distinguish cats from dogs are doing essentially the same thing we describe above: transforming raw inputs into some kind of latent representation (i.e. the final hidden layer of a network) where object classes are linearly separable. Neural networks also provide a useful sandbox to investigate manifold capacity theory.\n",
    "\n",
    "In this tutorial, we'll be implementing functions to compute the mean-field manifold capacity, radius, and dimension discussed in lecture and presented in detail in the [original paper]((https://journals.aps.org/prx/abstract/10.1103/PhysRevX.8.031003)). Tutorials 2 and 3 use these tools to analyze representations in neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d4b1e94",
   "metadata": {},
   "source": [
    "## 1.2 - Mean field theory of manifold capacity\n",
    "\n",
    "The original paper lays out a detailed and mathematically rigorous theory of manifold capacity using statistical mean field theory, the details of which are well beyond the scope of this tutorial. We will take the mean-field results for granted and work on implementing the calculations in python.\n",
    "\n",
    "The basic model considers a collection of $P$ manifolds $\\{M_\\mu\\}_{\\mu=1}^P$ embedded in $\\mathbb{R}^N$, each of which consists of a compact subset of a $D$-dimensional affine subspace of $\\mathbb{R}^N$, with $D \\ll N$. Each is represented by a point cloud $M^\\mu = \\{\\mathbf{x}^\\mu_i\\}_{i=1}^{n_\\mu}$, and we parametrize points on the manifold in terms of an orthonormal basis for the subspace containing $M^\\mu$:\n",
    "\n",
    "$$\\mathbf{x}^\\mu = \\sum_{i=1}^{D+1}s_i^\\mu\\mathbf{u}_i = \\mathbf{x}_0^\\mu + \\sum_{i=1}^{D}s_i^\\mu\\mathbf{u}_i$$\n",
    "\n",
    "This setup is visualized in the picture below:\n",
    "\n",
    "<p align='center'>\n",
    "    <img src='images/manifold_model.png' width='50%'>\n",
    "</p>\n",
    "\n",
    "We use the notation $\\vec{S} = (S_1,S_2,\\ldots,S_{D+1}) \\in \\mathbb{R}^{D+1}$ to denote a point on a manifold in its $D+1$-dimensional basis, and $\\mathcal{S} = \\{\\vec{S}_1,\\vec{S}_2,\\ldots,\\vec{S}_n\\}$ for the set of points associated to a manifold. Note that we need $D+1$ coordinates to describe $D$-dimensional subspace because it is not generally centered at the origin.\n",
    "\n",
    "We're interested in computing the \"manifold capacity\" $\\alpha$, which roughly translates to the maximal number of manifolds per dimension that are linearly separable in $\\mathbb{R}^N$ ($P/N$). This turns out to be a pretty complex problem in general, but analyzing it in the thermodynamic limit $P,\\,N \\to \\infty$ with a fixed load $\\alpha = P/N$ makes the problem tractable. This analysis yields a mean field theory for computing three quantities for a point cloud representing a manifold in $\\mathbb{R}^N$:\n",
    "\n",
    "1. **Manifold Capacity** $\\alpha_M$: the maximum number of such manifolds per dimension ($P/N$) that can be linearly separated in $\\mathbb{R}^N$.\n",
    "2. **Manifold Dimension** $D_M$: The intrinsic dimension of the manifold\n",
    "3. **Manifold Radius** $R_M$: The \"size\" or extent of the manifold in the ambient space.\n",
    "\n",
    "That's about as much as we need to know, but I encourage you to read the paper for more details! **Side note**: for visualization/conceptual purposes, it's easier to think of a manifold as the convex hull of its point cloud. These are the objects we're trying to separate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "556b56af",
   "metadata": {},
   "source": [
    "## 1.3 - Preprocessing\n",
    "\n",
    "Our collection of manifolds comes to us as a set of point clouds $\\{X_1, \\ldots, X_n\\}$ where $X_k \\in \\mathbb{R}^{N \\times P_k}$ ($P_k =$ number of points in manifold $k$). The first thing we'll implement is some basic **preprocessing** of each point cloud, which will make the capacity calculations a little easier. This consists of two steps:\n",
    "1. Center all points across all manifolds about the global mean of all the points in $\\mathbb{R}^N$: just like you would center your data before training a neural network.\n",
    "2. For each manifold, compute a $D+1$-dimensional basis for the affine subspace it lives in, and express points on the manifold in this basis (i.e. the $\\vec{S}$ notation used above)\n",
    "\n",
    "For the manifold bases, we will follow the convention of the original capacity paper and set the $D+1$-th basis vector to be the manifold's center $\\mathbf{c}$ (i.e. the mean of the point cloud for the manifold). We'll then express the remaining $1,\\ldots,D$ orthonormal basis vectors relative to this center. The coordinates for the center $\\mathbf{c}$ will be $\\vec{C} = (0,0,\\ldots,0,1)$, while the coordinates for any point $\\vec{S}$ will be $\\vec{S} = (S_1,S_2,\\ldots,S_D,1)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93f2a95d",
   "metadata": {},
   "source": [
    "Let's start with normalization. We want to do two things:\n",
    "1. **Global centering**: described above; shift all points across all manifolds such that their global mean is zero.\n",
    "\n",
    "2. **Local centering/normalization for each manifold**: to make downstream computations easier, we'll use \"local\" coordinates for each manifold. We want to independently shift each point cloud to have zero mean, and scale each point by the norm of its center location. In other words, for each point cloud we do: $$\\mathbf{x}_i \\mapsto \\frac{\\mathbf{x}_i - \\mathbf{c}}{||\\mathbf{c}||}, \\quad \\mathbf{c} = \\frac{1}{P}\\sum_{i=1}^{P}\\mathbf{x}_i$$\n",
    "\n",
    "#### **FILL IN CODE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6048d7aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def center_manifolds(X):\n",
    "    \"\"\"\n",
    "    Preprocess the set of manifolds by centering all the points about the origin, then center and normalize\n",
    "    the points of each manifold w.r.t to its center location/norm.\n",
    "    \n",
    "    Args:\n",
    "        X: list[np.ndarray]: list of point clouds, each a numpy array of shape (N, P_i) where N is the\n",
    "        dimensionality of the ambient space and P_i is the number of points in the i-th manifold.\n",
    "        \n",
    "    Returns:\n",
    "        list[np.ndarray]: Preprocessed manifolds\n",
    "        list[float]: List of norms of the centers of each manifold.\n",
    "    \"\"\"\n",
    "    # If we have more than one manifold point cloud, \n",
    "    # compute the global mean vector across all point clouds\n",
    "    # and center all the points about this global mean.\n",
    "    if len(X) > 1:\n",
    "        # Compute the mean across all manifolds\n",
    "        X_global_mean = ... # Your code here\n",
    "\n",
    "        # center all manifolds about the global mean\n",
    "        X_centered = [...] # Your code here\n",
    "    else:\n",
    "        X_centered = X\n",
    "    \n",
    "    # Compute centers (means) for each manifold and center/normalize the points of each manifold around its center\n",
    "    manifold_means = [...] # Your code here\n",
    "    manifold_mean_norms = [...] # Your code here\n",
    "    X_normalized = [...] # Your code here\n",
    "    \n",
    "    return X_normalized, manifold_mean_norms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f3193ea",
   "metadata": {},
   "source": [
    "Next, we compute orthonormal bases for each centered/normalized manifold. There are two relevant cases:\n",
    "1. $P_i \\geq N$: there are more points than ambient dimensions, in which case the standard basis in $\\mathbb{R}^N$ will suffice.\n",
    "\n",
    "2. $P_i < N$: there are fewer points than ambient dimensions, in which case our point cloud vectors span an at-most $P_i$-dimensional subspace of $\\mathbb{R}^N$. We want to compute an orthonormal basis $(\\mathbf{e}_1,\\ldots,\\mathbf{e}_D)$ for this space and express the points in $X_i$ in those coordinates. We can use the [**QR decomposition**](https://en.wikipedia.org/wiki/QR_decomposition), which decomposes a matrix $A \\in \\mathbb{R}^{m \\times n},$ with $m \\geq n$ into $A = QR$, where $Q$ is an $m \\times n$ matrix with orthonormal columns, and $R$ is an upper triangular $n \\times n$ matrix. This is essentially just the [Gram-Schmidt orthonormalization](https://en.wikipedia.org/wiki/Gram%E2%80%93Schmidt_process) of the columns of $A$, and thankfully `np.linalg.qr` can do it for us.\n",
    "\n",
    "#### **FILL IN CODE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb77b740",
   "metadata": {},
   "outputs": [],
   "source": [
    "def project_to_manifold_bases(X):\n",
    "    \"\"\"\n",
    "    Compute an orthonormal basis for each manifold using QR decomposition. Express points on each manifold in this basis, and add \n",
    "    an additional dimension for the center of the manifold. Each point should have coordinates of the form (S_1, S_2, ..., S_D, 1) \n",
    "    where D is the dimension of the manifold, and the last coordinate is always 1 (for the center).\n",
    "\n",
    "    Args:\n",
    "        X: list[np.ndarray]: list of point clouds, each a numpy array of shape (N, P_i). These should be the preprocessed manifold data.\n",
    "\n",
    "    Returns:\n",
    "        list[np.ndarray]: list of point clouds in their new orthonormal bases.\n",
    "    \"\"\"\n",
    "    X_new_basis = []\n",
    "    for x in X:\n",
    "        # Get dimensions/number of points in ambient space\n",
    "        N, P_i = x.shape\n",
    "        if N > P_i:\n",
    "            # Perform QR decomposition\n",
    "            Q, R = np.linalg.qr(x,mode='reduced') # Q has shape \n",
    "            # Project the points onto the orthonormal basis\n",
    "            x = ... # Your code here\n",
    "        \n",
    "        # add the center coordinate (1) as an aditional dimension for each point\n",
    "        N, new_dim = x.shape\n",
    "        x = np.concatenate((x, np.ones((1, new_dim))), axis=0)\n",
    "        # record the coordinates in the new basis\n",
    "        X_new_basis.append(x)\n",
    "    return X_new_basis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e97ba481",
   "metadata": {},
   "source": [
    "## 1.4 - How to calculate $\\alpha_M$\n",
    "\n",
    "Now we're ready to do some capacity calculations! Capacity generally depends on the desired sepration margin $\\kappa$ between manifolds, though we usually only consider the zero-margin setting $\\kappa = 0$ (i.e. a random dichotomy of the manifolds is \"just barely separable\"). Despite this, we will implement our code for the general case $\\kappa \\geq 0$.\n",
    "\n",
    "The key result from mean field theory is the following expression for the manifold capacity:\n",
    "\n",
    "$$\\alpha_M^{-1} = \\langle F(\\vec{T}) \\rangle_{\\vec{T}}$$\n",
    "\n",
    "where the average is performed over random $D+1$-dimensional vectors $\\vec{T} \\sim \\mathcal{N}(\\mathbf{0}_{D+1},\\mathbf{I}_{D+1})$, and\n",
    "\n",
    "$$\\begin{equation}F(\\vec{T}) = \\min_{\\vec{V}}\\{ ||\\vec{V} - \\vec{T}||^2 \\, | \\, \\vec{V}\\cdot\\vec{S} - \\kappa \\geq 0, \\; \\forall \\vec{S} \\in \\mathcal{S}\\}.\\end{equation}$$\n",
    "\n",
    "Recall that $\\mathcal{S}$ simply denotes the points on the manifold in terms of their $D+1$-dimensional coordinates."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd97e6c5",
   "metadata": {},
   "source": [
    "This seems a bit tricky to solve, but thankfully there useful tools. If we squint our eyes a bit we can immediately recognize that our problem resembles the [standard form](https://en.wikipedia.org/wiki/Optimization_problem#Continuous_optimization_problem) for constrained optimization:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\text{min}&\\text{imize } f(\\mathbf{x}) \\\\\n",
    "\\text{sub}&\\text{ject to } \\\\\n",
    "&g_i(\\mathbf{x}) \\leq 0 \\\\\n",
    "&h_j(\\mathbf{x}) = 0\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "where $i=1,\\ldots,m$ and $j=1,\\ldots,\\ell$ enumerate the constraint functions. In our case, we have $|\\mathcal{S}|$ constraints for each $\\vec{S}_i$ in the manifold:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "f(\\vec{V}) &= ||\\vec{V} - \\vec{T}||^2 \\\\\n",
    "g_i(\\vec{V}) &= -\\vec{S}_i\\cdot\\vec{V} + \\kappa, \\; i=1,\\ldots,|\\mathcal{S}|\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Before we get into solving this, we note there are two distinct cases, one of which leads to an easy solution:\n",
    "\n",
    "1. $\\vec{T} \\cdot \\vec{S} - \\kappa \\geq 0 \\; \\forall \\vec{S} \\in \\mathcal{S}$ -- in this case $\\vec{V} = \\vec{T}$ is a valid and optimal solution! We can check for this case by checking $\\min\\{\\vec{T}\\cdot\\vec{S} \\,| \\, \\vec{S} \\in \\mathcal{S}\\} - \\kappa \\geq 0$.\n",
    "2. The opposite of the above, i.e. $\\vec{T} \\cdot \\vec{S} - \\kappa < 0$ for some $\\vec{S}$. In this case, we'll actually need to solve the optimization problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfd3a544",
   "metadata": {},
   "source": [
    "### The KKT conditions for constrained optimization\n",
    "As discussed in the [capacity paper](https://journals.aps.org/prx/abstract/10.1103/PhysRevX.8.031003), the [Karush-Kuhn-Tucker (KKT) conditions](https://en.wikipedia.org/wiki/Karush%E2%80%93Kuhn%E2%80%93Tucker_conditions) are a helpful way to characterize the solution $\\vec{V}^\\ast$ to the optimization problem. We're interested in finding a saddle point of the following [Lagrangian function](https://en.wikipedia.org/wiki/Lagrange_multiplier):\n",
    "\n",
    "$$\n",
    "\\mathcal{L}(\\vec{V},\\boldsymbol{\\lambda}) = f(\\mathbf{x}) + \\sum_{i=1}^{|\\mathcal{S}|}\\lambda_i g_i(\\mathbf{x}).\n",
    "$$\n",
    "\n",
    "The KKT conditions state that the solution $(\\vec{V}^\\ast,\\boldsymbol{\\lambda}^\\ast)$ satisfies the following conditions:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "&\\nabla f(\\vec{V}^\\ast) + \\sum_{i=1}^{|\\mathcal{S}|}\\lambda_i^\\ast \\nabla g_i(\\vec{V}^\\ast) = 0 \\\\\n",
    "&g_i(\\vec{V}^\\ast) \\leq 0 \\\\\n",
    "&\\lambda_i^\\ast \\geq 0 \\\\\n",
    "&\\lambda_i^\\ast g_i(\\vec{V}^\\ast) = 0\n",
    "\\end{align*}\n",
    "$$\n",
    "where last three hold for all $i=1,\\ldots,|\\mathcal{S}|$. To make the analysis simpler, we consider minimizing $\\tilde{f}(\\vec{V}) = \\frac{1}{2}||\\vec{V}||^2 - \\vec{V}\\cdot\\vec{T}$ instead of $f(\\vec{V}) = ||\\vec{V}-\\vec{T}||^2$, because they both have the same optimum (**exercise**: check this). The first KKT condition then reads:\n",
    "\n",
    "$$\\vec{V}^\\ast - \\vec{T} - \\sum_{i=1}^{|\\mathcal{S}|}\\lambda_i^\\ast \\vec{S}_i = 0,$$\n",
    "\n",
    "which we can rearrange as\n",
    "\n",
    "$$\\vec{V}^\\ast = \\vec{T} + \\sum_{i=1}^{|\\mathcal{S}|}\\lambda_i^\\ast \\vec{S}_i.$$\n",
    "\n",
    "If we now define $\\Lambda = \\sum \\lambda_i^\\ast$ and $\\tilde{\\lambda}_i^\\ast = \\lambda_i^\\ast/\\Lambda$, we can write:\n",
    "\n",
    "$$ \\vec{V}^\\ast = \\vec{T} + \\Lambda \\sum_{i=1}^{|\\mathcal{S}|}\\tilde{\\lambda}_i^\\ast \\vec{S}_i = \\vec{V}^\\ast + \\Lambda \\tilde{S}(\\vec{T})$$\n",
    "\n",
    "Why have we done this? If you look closely, you'll notice $\\tilde{S}(\\vec{T})$ is on the **convex hull** of our manifold. We call this an **anchor point** of the manifold for the sample $\\vec{T}$. What the KKT conditions tell us is that we can decompose our solution $\\vec{V}^\\ast$ into $\\vec{T}$ and an anchor point on the convex hull scaled by $\\Lambda$. These anchor points turn out to be exactly what we need to compute the manifold capacity, radius, and dimension.\n",
    "\n",
    "First, let's make a few useful observations:\n",
    "1. Our decomposition of $\\vec{V}^\\ast$ allows us to write $$ F(\\vec{T}) = ||\\Lambda\\tilde{S}(\\vec{T})||^2, $$ directly tying the calculation of manifold capacity $\\alpha_M^{-1} = \\langle F(\\vec{T}) \\rangle$ to the calculation of these anchor points.\n",
    "\n",
    "2. We wrote our optimization problem with $|\\mathcal{S}|$ constraints, but we could have written it with just **one** constraint defined by the **support function** $$g_\\mathcal{S}(\\vec{V}) = \\min_{\\vec{S} \\in \\mathcal{S}} \\vec{V}\\cdot \\vec{S} \\implies \\text{constraint: } g_\\mathcal{S}(\\vec{V}) - \\kappa \\geq 0.$$ This looks simpler but sadly support functions are not differentiable everywhere, so we need to replace the gradients in the first KKT condition with [subgradients](https://en.wikipedia.org/wiki/Subderivative) (which are equal to the usual gradient when $g_\\mathcal{S}$ is differentiable). Under this variant of the problem, the KKT conditions tell us: $$ \\vec{V}^\\ast = \\vec{T} + \\mu \\partial g_S(\\vec{V}^\\ast),$$ where $\\partial$ denotes a subgradient and $\\mu$ is a *single* Lagrange multiplier. Comparing this to what we've just done, we can identify $\\mu = \\Lambda = \\sum\\lambda_i^\\ast$ and $\\tilde{S}(\\vec{T}) = \\partial g_\\mathcal{S}(\\vec{V}^\\ast)$ -- in other words, the anchor point is the unique subgradient of $g_\\mathcal{S}$ satisfying the KKT conditions! **When $g_\\mathcal{S}$ is differentiable**, we can thus identify: $$ \\tilde{S}(\\vec{T}) = \\argmin_{\\vec{S} \\in \\mathcal{S}} \\vec{V}^\\ast \\cdot \\vec{S}$$\n",
    "\n",
    "3. If we are in the \"easy\" case $\\min\\{\\vec{T}\\cdot\\vec{S} \\,| \\, \\vec{S} \\in \\mathcal{S}\\} - \\kappa \\geq 0$ where we immediately see $\\vec{V} = \\vec{T}$ is the solution and thus don't need to solve the optimization problem, we can still identify the anchor point as $\\tilde{S}(\\vec{T}) = \\partial g_\\mathcal{S}(T) = \\argmin_{\\vec{S}\\in\\mathcal{S}}\\vec{T}\\cdot\\vec{S}$\n",
    "\n",
    "4. If we are not in the trivial case, we can solve the optimization problem for $(\\vec{V}^\\ast,\\boldsymbol{\\lambda}^\\ast)$ and compute the anchor vector as: $$\\tilde{S}(\\vec{T}) = \\frac{\\vec{V}^\\ast - \\vec{T}}{\\Lambda}$$\n",
    "\n",
    "5. Outside the trivial case, there exists a self-consistent equation for $\\Lambda$ ($\\mu$) (the details are beyond the scope of this tutorial): $$ \\Lambda = \\frac{\\max(-\\vec{T}\\cdot \\tilde{S}(\\vec{T}) + \\kappa, 0)}{||\\tilde{S}(\\vec{T})||^2} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a27278c",
   "metadata": {},
   "source": [
    "### Computing Capacity, Radius, and Dimension\n",
    "These anchor vectors turn out to be very useful: they are the ingredients we need for computing manifold capacity, radius, and dimension. They are defined as follows:\n",
    "\n",
    "1. **Capacity**: $\\alpha_M^{-1} = \\langle ||\\Lambda \\tilde{S}(\\vec{T})||^2 \\rangle_{\\vec{T}}$\n",
    "\n",
    "2. **Radius**: $R_M^2 = \\langle ||\\tilde{s}||^2 \\rangle_{\\vec{T}}$\n",
    "\n",
    "3. **Dimension**: $D_M = \\langle (\\vec{t}\\cdot \\hat{s})^2 \\rangle_{\\vec{T}}$\n",
    "\n",
    "where $\\tilde{s}$ and $\\vec{t}$ refer to vectors of the first $D$ coordinates of $\\tilde{S}$ and $\\vec{T}$ excluding the $D+1$-th coordinate that specifies the center, and $\\hat{s}$ is a unit vector in the direction of $\\tilde{s}$. The computation of capacity uses the full vector $\\tilde{S}$.\n",
    "\n",
    "$R_M$ and $D_M$ both have nice intuitive interpretations related to the manifold's geometric structure. $R_M$ is simply the mean squared norm of the anchor points -- i.e. points on the manifold or its convex hull -- and naturally measures the manifold's spatial extent. $D_M$ essentially measures the average cosine between a random direction $\\vec{T}$ in $\\mathbb{R}^D$ and the corresponding anchor point on the manifold. If most of the time there is an anchor point roughly along the same direction as a randomly sampled $\\vec{t}$, the manifold \"fills all $D$ dimensions\" and $D_M \\approx \\langle ||\\vec{t}||^2 \\rangle = D$. Otherwise, $D_M$ will generically be lower than $D$ depending on the manifold's intrinsic dimensionality (note that $D_M$ is upper bounded by $D$)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76d12ee6",
   "metadata": {},
   "source": [
    "## 1.5 - Finally, some code again!\n",
    "\n",
    "We might be in danger of losing the forest for the trees here, but don't worry -- now we have all the tools we need! Earlier we wrote functions to compute the centered/normalized representation of a manifold $X_k \\in \\mathbb{R}^{D+1 \\times P_i}$ in its local $D+1$-dimensional coordinate system. Starting with this, we compute capacity/radius/dimension calculations in three steps:\n",
    "\n",
    "1. Randomly sample a bunch of $\\vec{T} \\in \\mathcal{N}(\\mathbf{0}_{D+1},\\mathbf{I}_{D+1})$\n",
    "2. For each $\\vec{T}$, compute the anchor point $\\tilde{S}(\\vec{T})$ on the manifold (this is the constrained minimization problem we talked at length about)\n",
    "3. Compute $\\alpha_M$, $R_M$, $D_M$ as above.\n",
    "\n",
    "Step 1 is easy, so let's get to work coding up step 2!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adc6073a",
   "metadata": {},
   "source": [
    "### Computing anchor points\n",
    "\n",
    "Let's implement the following algorithm:\n",
    "\n",
    "**Given**: Manifold point cloud $X = (\\vec{S}_1,\\ldots,\\vec{S}_P) \\in \\mathbb{R}^{D+1 \\times P}$ and a randomly sampled $\\vec{T} \\sim \\mathcal{N}(\\mathbf{0}_{D+1},\\mathbf{I}_{D+1})$\n",
    "\n",
    "**Goal**: Determine the minimizer $\\vec{V}^\\star$ for the function $f(\\vec{V}) = ||\\vec{V} - \\vec{T}||^2$ with the constraint $X^T\\vec{V} - \\kappa \\geq \\vec{0}$\n",
    "\n",
    "**Steps**:\n",
    "\n",
    "* If $\\min\\{\\vec{T} \\cdot \\vec{S} - \\kappa \\; | \\; \\vec{S} \\in \\mathcal{S}\\} \\geq 0$:\n",
    "    * $\\vec{V} = \\vec{T}$ is the minimizer and satisfies the constraint.\n",
    "    * $\\tilde{S}(\\vec{T}) = \\argmin_{\\vec{S} \\in \\mathcal{S}}\\vec{V}\\cdot \\vec{S}$\n",
    "* Otherwise:\n",
    "    * Use `cvxopt` to find the minimizer $\\vec{V}^\\ast$ and the associated Lagrange multipliers $\\lambda^\\ast_i$ for $i=1,\\ldots,P$\n",
    "    * Compute $\\tilde{S}(\\vec{T}) = (\\vec{V}^\\ast - \\vec{T})/\\sum_{i} \\lambda_i^\\ast$\n",
    "    * Compute $\\Lambda = \\max(-\\vec{T}\\cdot\\tilde{S}(\\vec{T}) + \\kappa,0)/||\\tilde{S}(\\vec{T})||^2$\n",
    "\n",
    "In practice, we will use `numpy` vectorized operations to do this for a whole batch of sampled $\\vec{T}$ at once.\n",
    "\n",
    "#### **FILL IN CODE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8781c0d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def samples_and_anchor_points(S, kappa, num_t_samples=300):\n",
    "    \"\"\"\n",
    "    Prepare for the capacity/dimension/radius calculations by sampling num_t_samples random vectors T and computing the anchor points for each\n",
    "    \n",
    "    Args:\n",
    "        S: np.ndarray: shape (D+1,P_i) -- The point cloud representing the manifold in its centered/normalized basis coordinates\n",
    "                                          S = (S_1, S_2, ..., S_P) where S_i is a D+1-dim vector.\n",
    "        \n",
    "        kappa: float: the manifold capacity separation margin.\n",
    "        \n",
    "        num_t_samples: int: the number of random vectors T to sample; 300 is a reasonable default\n",
    "\n",
    "    Returns:\n",
    "        T_samples: np.ndarray: shape (D+1, num_t_samples) -- the randomly sampled vectors T in R^(D+1)\n",
    "        S_tildes: np.ndarray: shape (D+1, num_t_samples) -- The anchor points for each sampled vector in T_samples\n",
    "    \"\"\"\n",
    "    \n",
    "    D1, m = S.shape # D1 = D+1, m = number of points in the manifold\n",
    "    D = D1 - 1 # D is the dimension of the manifold without its center coordinate\n",
    "    \n",
    "    # Sample num_t_samples random vectors T of dimension D1 from a standard normal distribution\n",
    "    T_samples = ... # Your code here, array of shape (D1, num_t_samples)\n",
    "\n",
    "    # compute the anchor points for each T sample\n",
    "    S_tildes = compute_anchor_points(S,T_samples,kappa) # implement this function below\n",
    "\n",
    "    return T_samples, S_tildes\n",
    "\n",
    "def compute_anchor_points(S, T_samples, kappa, eps=1e-8):\n",
    "    \"\"\"\n",
    "    For each sampled vector in T, compute the anchor point S_tilde. This involves solving min_V (||V - T||^2 | V.s - kappa >= 0 for all s in S) for each t in T.\n",
    "    \n",
    "    Args:\n",
    "        S: np.ndarray: (D+1, P_i) -- The point cloud representing the manifold in its basis coordinates, shape (D+1,P_i).\n",
    "        T_samples: np.ndarray: (D+1, N_T) -- a collection of num_t_samples vectors in R^D+1, sampled from a standard normal distribution.\n",
    "        kappa: the manifold capacity separation margin.\n",
    "\n",
    "    Returns:\n",
    "        S_tildes: np.ndarray: (D+1, num_t_samples) -- The anchor points for each sampled vector in T_samples\n",
    "    \"\"\"\n",
    "\n",
    "    num_t_samples = T_samples.shape[1]  # number of random T samples\n",
    "\n",
    "    # For each T, compute its minimum projection onto the points in S -- this will help us determine if there is an easy solution V = T\n",
    "    projections, S_min_projections = min_projection(T_samples, S) # implement this function below\n",
    "\n",
    "    # for each random T, compute the anchor point s_tilde and collect them together\n",
    "    S_tildes = [] \n",
    "    for i in range(num_t_samples):\n",
    "        T = T_samples[:, i]\n",
    "        \n",
    "        # check if there is a V = T solution, by checking the min projection\n",
    "        if projections[i] - kappa >= 0:\n",
    "            Vstar = ... # Your code here\n",
    "            s_tilde = ... # Your code here\n",
    "        else:\n",
    "            # compute the optimal V using a solver routine (below)\n",
    "            Vstar, lambdas, min_VT2 = minimize_VminusT2(T, S, kappa) # go fill in this function (most of it is done for you)\n",
    "\n",
    "            if np.linalg.norm(Vstar - T) < eps: \n",
    "                # if the solution is really close to T, it's probably the \"easy\" case in reality and numerical oddities happened\n",
    "                # this should be exactly what you did in the first if case above\n",
    "                Vstar = ...\n",
    "                s_tilde = ...\n",
    "            else:\n",
    "                # use Vstar and the Lagrange multiplier alpha to compute the anchor point s_tilde\n",
    "                Lambda = np.sum(lambdas) # the sum of the Lagrange multipliers\n",
    "                s_tilde = ... # Your code here\n",
    "        \n",
    "        S_tildes.append(s_tilde.reshape(-1,1)) # reshape to (D+1, 1) for each anchor point\n",
    "    \n",
    "    # concat S_tildes to be a (D+1, num_t_samples) array\n",
    "    S_tildes = np.concatenate(S_tildes,axis=1)\n",
    "\n",
    "    return S_tildes\n",
    "\n",
    "\n",
    "def min_projection(T, S):\n",
    "    \"\"\"\n",
    "    Compute the minimum projection of each vector of T onto the points in S.\n",
    "    \n",
    "    Args:\n",
    "        T: np.ndarray: (D+1,N_T) -- a collection of N_T vectors in R^D+1\n",
    "        S: np.ndarray: (D+1,P_i) -- the point cloud representing the manifold in its basis coordinates\n",
    "\n",
    "    Returns:\n",
    "        min_projections: np.ndarray: (N_T,) -- the minimum projection of each vector in T onto the points in S\n",
    "        S_min_projections: np.ndarray: (D+1,N_T) -- the points in S which correspond to the minimum projections\n",
    "    \"\"\"\n",
    "    # compute a matrix M_ij = T_i dot S_j, where T_i is the i-th vector in T and S_j is the j-th point in S\n",
    "    # should  have shape (N_T, P_i)\n",
    "    M = ... # Your code here\n",
    "\n",
    "    # get the minimum projection for each vector in T\n",
    "    min_projections = ... # Your code here\n",
    "\n",
    "    # retrieve the vectors in S which correspond to the minimum projections (hint: use argmin)\n",
    "    S_min_projections = S[...] # Your code here\n",
    "\n",
    "    return min_projections, S_min_projections\n",
    "\n",
    "def minimize_VminusT2(T, S, kappa):\n",
    "    \"\"\"\n",
    "    Minimize the function ||V - T||^2 subject to the constraint V.s - kappa >= 0 for all s in S. We use the cvxopt library to solve this constrained optimization problem.\n",
    "    As discussed in the theory section, we actually minimize 0.5V^2 - V.T\n",
    "\n",
    "    This function is mostly filled in for you. You just need to prepare the matrices for cvxopt \n",
    "    \n",
    "    Here's the prolem setup:\n",
    "        cvxopt wants the problem in the form of a quadratic program:\n",
    "            minimize 0.5 x^T P x + q^T x\n",
    "            subject to Gx <= h\n",
    "            where P and G are matrices\n",
    "        \n",
    "        In our case x = T and we have:\n",
    "            f(x) = 0.5 ||x||^2 - x . T\n",
    "            subject to V.s - kappa >= 0 for all s in S (i.e. s = columns of S which is of shape (D+1, P))\n",
    "            \n",
    "            P = I (identity matrix)\n",
    "            q = -T\n",
    "        \n",
    "        We write our constraint as:\n",
    "            matmul(S_transpose, V) - kappa >= 0 (kappa = a vector of shape (P,) with all entries equal to kappa)\n",
    "            --> matmul(-S_transpose, V) <= -kappa\n",
    "            which means:\n",
    "                G = -S_transpose \n",
    "                h = -kappa\n",
    "        \n",
    "    \n",
    "    Args:\n",
    "        T: np.ndarray: (D+1,) -- a vector in R^D+1\n",
    "        S: np.ndarray: (D+1,P_i) -- the point cloud representing the manifold in its basis coordinates\n",
    "        kappa: float -- the separation margin\n",
    "\n",
    "    Returns:\n",
    "        Vstar: np.ndarray: (D+1,) -- the optimal vector V which minimizes ||V - T||^2 subject to the constraint\n",
    "        alpha: float -- the Lagrange multiplier associated with the constraint\n",
    "        min_VT2: float -- the minimum value of the original objective function ||V - T||^2\n",
    "    \"\"\"\n",
    "    D1 = T.shape[0]\n",
    "    m = S.shape[1] \n",
    "\n",
    "    # set up the quadratic program\n",
    "    ## FILL IN CODE HERE ##\n",
    "    P = cvxopt.matrix(...)  # identity matrix of shape (D1,D1)\n",
    "    q = cvxopt.matrix(...) # q = -T\n",
    "    G = cvxopt.matrix(...) # G = -S_transpose\n",
    "    h = cvxopt.matrix(...) # h = -kappa (*vector* of shape (m,) not a scalar!)\n",
    "\n",
    "    # solve the quadratic program\n",
    "    sol = cvxopt.solvers.qp(P, q, G, h)\n",
    "\n",
    "    # retrieve the outputs we want: Vstar and the Lagrange multiplier alpha\n",
    "    Vstar = np.array(sol['x']).flatten() # optimal V\n",
    "    lambdas = np.array(sol['z']).flatten() # Lagrange multipliers\n",
    "    min_VT2 = np.sum((Vstar-T)**2) # the minimum value of the original objective function ||V-T||^2\n",
    "\n",
    "    return Vstar, lambdas, min_VT2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88e9aab4",
   "metadata": {},
   "source": [
    "With samples and anchor points in hand, let's implement routines to compute $\\alpha_M$, $R_M$ and $D_M$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5fe22c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_capacity(T_samples, S_tildes, kappa):\n",
    "    \"\"\"\n",
    "    Compute the manifold capacity alpha_M = 1 / <F(T)>, where F(T) = ||Lambda * S_tilde(T)||^2 \n",
    "\n",
    "    Args:\n",
    "        T_samples: np.ndarray: (D+1, num_t_samples) -- a collection of num_t_samples vectors in R^D+1, sampled from a standard normal distribution.\n",
    "        S_tildes: np.ndarray: (D+1, num_t_samples) -- the anchor points for each sampled vector in T_samples\n",
    "        kappa: float -- the manifold capacity separation margin\n",
    "    \"\"\"\n",
    "    # Computing Lambda = max(-T . S_tilde + kappa, 0) / ||S_tilde||^2\n",
    "\n",
    "    # first compute a vector of ||S_tilde||^2 for each s_tilde (for the denominator); shape (num_t_samples,)\n",
    "    S_tilde_normSquared = ... # Your code here\n",
    "\n",
    "    # now compute the Lambda values\n",
    "    lam = ... # Your code here; shape (num_t_samples,)\n",
    "\n",
    "    # compute F(T) = (Lambda^2) * ||S_tilde||^2\n",
    "    FT = ... # Your code here; F(T) for each T; shape (num_t_samples,)\n",
    "\n",
    "    # compute the manifold capacity alpha_M = 1 / <F(T)>\n",
    "    alpha_M = ...\n",
    "\n",
    "    return alpha_M\n",
    "\n",
    "def compute_radius(S_tildes):\n",
    "    \"\"\"\n",
    "    Compute the radius of the manifold, which is the average norm of the first D coordinates of the anchor points S_tildes.\n",
    "    \n",
    "    Args:\n",
    "        S_tildes: np.ndarray: (D+1, num_t_samples) -- the anchor points for each sampled vector in T_samples\n",
    "\n",
    "    Returns:\n",
    "        radius: float -- the radius of the manifold\n",
    "    \"\"\"\n",
    "    D1, nsamples = S_tildes.shape  # D1 = D + 1\n",
    "    D = D1 - 1\n",
    "\n",
    "    # account for any nonzero mean we might have among the anchor points, even though the manifold mean overall should be zero\n",
    "    mean_S = np.mean(S_tildes,axis=1,keepdims=True)\n",
    "    center_S = S_tildes[-1, :] # the center coordinate of each anchor point (in theory this is always 1, in practice it might not be, so we scale our vectors by it)\n",
    "    S_centered_scaled = (S_tildes - mean_S) / center_S\n",
    "    \n",
    "    # take just the first D coordinates of each anchor point\n",
    "    s = ... # your code here\n",
    "\n",
    "    # compute the R_M = sqrt(<||s||^2>)\n",
    "    R_M = ... # your code here\n",
    "\n",
    "    return R_M\n",
    "\n",
    "def compute_dimension(T_samples, S_tildes):\n",
    "    \"\"\"\n",
    "    Compute the dimension of the manifold, D = <(s . t)^2> \n",
    "    \n",
    "    Args:\n",
    "        T_samples: np.ndarray: (D+1, num_t_samples) -- a collection of num_t_samples vectors in R^D+1, sampled from a standard normal distribution.\n",
    "        S_tildes: np.ndarray: (D+1, num_t_samples) -- the anchor points for each sampled vector in T_samples\n",
    "\n",
    "    Returns:\n",
    "        dimension: float -- the dimension of the manifold\n",
    "    \"\"\"\n",
    "    D1, nsamples = S_tildes.shape  # D1 = D + 1\n",
    "    D = D1 - 1\n",
    "\n",
    "    # grab the first D coordinates of S_tilde and T\n",
    "    s = ... # Your code here\n",
    "    t = ... # Your code here\n",
    "\n",
    "    # compute unit vectors\n",
    "    s_hat = ... # Your code here\n",
    "    t_hat = ... # Your code here\n",
    "\n",
    "    # compute the dimension D_M = D * <(s_hat . t_hat)^2>\n",
    "    D_M = ...\n",
    "\n",
    "    return D_M"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "988f2d17",
   "metadata": {},
   "source": [
    "We finally have functions to compute everything we want! Now let's wrap it all up into a single function\n",
    "\n",
    "Given a list of manifold point clouds $[X_1,\\ldots,X_m]$, our function will compute the capacity, radius, and dimension for each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0761ccae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_manifold_quantities(X, kappa, num_t_samples=300, return_anchor_points=False):\n",
    "    \"\"\"\n",
    "    Compute various manifold quantities from the input data.\n",
    "\n",
    "    Args:\n",
    "        X: list[np.ndarray] -- manifolds [X_1, ..., X_m], where each X_i is a manifold point cloud with shape (N, P_i) \n",
    "        kappa: float -- the manifold capacity separation margin\n",
    "        num_t_samples: int -- number of randomly sampled vectors to use in capacity calculation\n",
    "\n",
    "    Returns:\n",
    "        alpha_M: list[float] -- the manifold capacities computed for each manifold\n",
    "        R_M: list[float] -- the radii computed for each manifold\n",
    "        D_M: list[float] -- the dimensions computed for each manifold\n",
    "    \"\"\"\n",
    "    \n",
    "    # first center/normalize the data\n",
    "    X_centered_normed, mean_norms = center_manifolds(X)\n",
    "    \n",
    "    # express manifold point clouds in their D+1-dimensional bases\n",
    "    X_proj = project_to_manifold_bases(X_centered_normed)\n",
    "\n",
    "    # compute capacities, radii, and dimensions for each manifold\n",
    "    alpha_M = []\n",
    "    R_M = []\n",
    "    D_M = []\n",
    "    anchor_points = []\n",
    "    for Xp in X_proj:\n",
    "        T_samples, S_tildes = samples_and_anchor_points(Xp, kappa, num_t_samples)\n",
    "        \n",
    "        a = compute_capacity(T_samples, S_tildes, kappa)\n",
    "        R = compute_radius(S_tildes)\n",
    "        D = compute_dimension(T_samples, S_tildes)\n",
    "\n",
    "        alpha_M.append(a)\n",
    "        R_M.append(R)\n",
    "        D_M.append(D)\n",
    "        anchor_points.append(S_tildes)\n",
    "    alpha_M = np.array(alpha_M)\n",
    "    R_M = np.array(R_M) * np.array(mean_norms)  # scale the radii by the mean norms of the manifolds\n",
    "    D_M = np.array(D_M)\n",
    "    if return_anchor_points:\n",
    "        return alpha_M, R_M, D_M, anchor_points\n",
    "    else:\n",
    "        return alpha_M, R_M, D_M"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "476be624",
   "metadata": {},
   "source": [
    "## 1.6 - Exploring manifold capacity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f81b6b42",
   "metadata": {},
   "source": [
    "Now that we've got the tools we need, let's explore how manifold capacity works for a simple class of manifolds: **ellipsoids**. In $\\mathbb{R}^N$ these are defined by the equation\n",
    "\n",
    "$$ \\sum_{i=1}^N \\frac{x_i^2}{R_i^2} = 1$$\n",
    "\n",
    "where $R_i$ are the lengths of the various axes of the ellipse. \n",
    "\n",
    "We can generate points on an axis-aligned ellipsoid by generating points on the unit sphere, then dilating each dimension the corresponding $R_i$. Complete the code implementation below.\n",
    "\n",
    "#### **FILL IN CODE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b35175de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_unit_ball(n_samples, n_dim, surface=True):\n",
    "    \"\"\"\n",
    "    Uniformly sample points on the unit sphere or in the unit ball in R^n_dim, default is to sample on the surface (surface=True)\n",
    "\n",
    "    Args:\n",
    "        n_samples: number of samples\n",
    "        n_dim: dimension of the space\n",
    "\n",
    "    Returns:\n",
    "        samples: (n_samples, n_dim) array\n",
    "    \"\"\"\n",
    "    # Sample from standard normal shape (n_samples, n_dim)\n",
    "    x = ... # Your code here\n",
    "    # normalize each vector to lie on the unit sphere\n",
    "    x /= ... # Your code here\n",
    "\n",
    "    if not surface:\n",
    "    # Scale by r^(1/n) where r ~ Uniform[0, 1]\n",
    "        r = np.random.rand(n_samples) ** (1 / n_dim)\n",
    "        x *= r[:, None]\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba9ab686",
   "metadata": {},
   "source": [
    "Now complete the code to generate points on an ellipsoids with axis lengths $[R_1,R_2,\\ldots,R_N]$ and add a random offset $\\mathbf{c}$ as the center location.\n",
    "\n",
    "#### **FILL IN CODE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a8124f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_ellipsoid_points(n_points, axes_lengths, radius=None, rcenter=1.0, surface=True):\n",
    "    \"\"\"\n",
    "    Generate points on an N-dimensional ellipsoid with specified axis lengths.\n",
    "    \n",
    "    Args:\n",
    "        n_points (int): Number of points to generate\n",
    "        axes_lengths (list or numpy.ndarray): The lengths of the semi-axes of the ellipsoid\n",
    "        radius: float, optional: If specified, scale the axes lengths so that the longest one is equal to this radius\n",
    "        rcenter: float, optional: radius of the random center offset, defaults to 1\n",
    "        surface: bool, optional: If True, sample points on the surface of the ellipsoid; \n",
    "                                 if False, sample points uniformly in the volume of the ellipsoid.      \n",
    "        \n",
    "    Returns:\n",
    "        numpy.ndarray: Array of shape (n_points, N) containing points on the ellipsoid\n",
    "    \"\"\"\n",
    "    if type(axes_lengths) is not np.ndarray:\n",
    "        axes_lengths = np.array(axes_lengths)\n",
    "    \n",
    "    # Number of dimensions\n",
    "    N = len(axes_lengths)\n",
    "\n",
    "    points = sample_unit_ball(n_points, N)  # Generate points on the unit sphere\n",
    "    \n",
    "    # scale axes lengths so that the longest one is equal to radius\n",
    "    if radius is not None:\n",
    "        axes_lengths = radius * axes_lengths / np.max(axes_lengths)\n",
    "    \n",
    "    # Scale each dimension by its axis length to get an ellipsoid\n",
    "    points = ... # Your code here\n",
    "\n",
    "    # put the points around a random nonzero center sampled from the unit sphere\n",
    "    # sample a random center point on the unit sphere\n",
    "    center = sample_unit_ball(1, N)\n",
    "    # add the random center offset scaled by rcenter\n",
    "    points += rcenter * center \n",
    "\n",
    "    return points"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "468e38b7",
   "metadata": {},
   "source": [
    "Finally, let's implement a function that embeds a set of points (ellipsoid or otherwise) in $\\mathbb{R}^N$ into a higher-dimensional space $\\mathbb{R}^M$ ($M > N$). We'll be sampling relatively low-dimensional ellipsoids, embedding them into a high-dimensional ambient space, then using our manifold capacity tools to analyze their intrinsic dimension/packability. \n",
    "\n",
    "#### **FILL IN CODE**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21fa6bc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_to_dim(x,M):\n",
    "    nsamples,N = x.shape\n",
    "    assert M > N\n",
    "    \n",
    "    # randomly choose N distinct axes in R^M to embed the points in\n",
    "    rand_coords = ... # Your code here; should have shape N\n",
    "    \n",
    "    # initialize the embedding vector as zeros\n",
    "    embedding = np.zeros((nsamples,M))\n",
    "    \n",
    "    # assign the points to the randomly chosen axes\n",
    "    embedding[...] = x # Your code here\n",
    "    \n",
    "    return embedding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a89a417",
   "metadata": {},
   "source": [
    "Let's do a quick sanity check to make sure our code is working properly:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ac710c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "nsamp = 1000\n",
    "axes_lengths = [2, 10, 5]\n",
    "rcenter = 1.0\n",
    "points = generate_ellipsoid_points(nsamp, axes_lengths, rcenter=rcenter)\n",
    "\n",
    "# Visualize the generated points\n",
    "\n",
    "# 3D visualization\n",
    "fig = plt.figure(figsize=(8, 6))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "ax.scatter(points[:, 0], points[:, 1], points[:, 2], alpha=0.7, s=10)\n",
    "ax.set_xlabel('X')\n",
    "ax.set_ylabel('Y')\n",
    "ax.set_zlabel('Z')\n",
    "\n",
    "# Set appropriate axis bounds\n",
    "center = points.mean(axis=0)\n",
    "radius = max(axes_lengths)\n",
    "ax.set_xlim([center[0]-radius, center[0]+radius])\n",
    "ax.set_ylim([center[1]-radius, center[1]+radius])\n",
    "ax.set_zlim([center[2]-radius, center[2]+radius])\n",
    "\n",
    "ax.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "887c3d75",
   "metadata": {},
   "source": [
    "Now we can put our manifold capacity tools to use. In the cells below we'll generate some ellipsoids of varying dimension, embed them into a fixed high-dimensional space, compute the manifold capacity/dimension, and look at how these quantities reflect the ellipsoid's true dimension. **NOTE**: the manifold capacity code expects to receive a list of point clouds $[p_1,\\ldots,p_k]$ with shapes $p_i \\in \\mathbb{R}^{N\\times n_\\text{samples}}$, so you'll need to tranpose the point clouds we generate with our functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f16499e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# feel free to edit these choices!\n",
    "ambient_dim = 100\n",
    "ellips_dims = [2,4,6,8,10,15] \n",
    "axes_lengths = [d*[1] for d in ellips_dims]  # just do spheres; feel free to change this\n",
    "nsamples_per = 500\n",
    "rcenter = 1.0\n",
    "\n",
    "point_clouds = []\n",
    "for edim,ax_lengths in zip(ellips_dims,axes_lengths):\n",
    "    # generate points\n",
    "    points = generate_ellipsoid_points(nsamples_per, ax_lengths, rcenter=rcenter, radius=1.0)\n",
    "\n",
    "    # embed in a ambient_dim space\n",
    "    points = embed_to_dim(points, ambient_dim)\n",
    "\n",
    "    # add to the point cloud list (careful about the shape!)\n",
    "    point_clouds.append(points.T)\n",
    "\n",
    "alphas, Rs, Ds = compute_manifold_quantities(point_clouds, kappa=0, num_t_samples=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bc12927",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,axes = plt.subplots(1,3,figsize=(18,5))\n",
    "\n",
    "plt.sca(axes[0])\n",
    "plt.plot(ellips_dims, alphas, lw=2)\n",
    "plt.xlabel('Ellipsoid Dimension')\n",
    "plt.ylabel('Mean Field Capacity $\\\\alpha_M$')\n",
    "\n",
    "plt.sca(axes[1])\n",
    "plt.plot(ellips_dims, Ds, lw=2)\n",
    "plt.xlabel('Ellipsoid Dimension')\n",
    "plt.ylabel('Mean Field Dimension $D_M$')\n",
    "\n",
    "plt.sca(axes[2])\n",
    "plt.plot(ellips_dims, Rs, lw=2)\n",
    "plt.xlabel('Ellipsoid Dimension')\n",
    "plt.ylabel('Mean Field Radius $R_M$')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb6b34cd",
   "metadata": {},
   "source": [
    "#### **Exercises**\n",
    "Try varying a few parameters and exploring how the mean-field capacity, radius, and dimension change. A few things to consider\n",
    "\n",
    "1. Does manifold capacity $\\alpha_M$ vary in an intuitive way as you change vary the ellipsoid dimension and the ambient dimension?\n",
    "\n",
    "2. How is the number of samples per manifold vs. the ambient dimension reflected in the \"accuracy\" of the computed dimension/radius (relative to ground truth)?\n",
    "\n",
    "3. Are $R_M$/$D_M$ systematically over/underestimated? Under what conditions?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6ed3fa1",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "iaifi25",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
